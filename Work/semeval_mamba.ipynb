{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vouVUAXOG_IE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# 1. Installation Cell\n",
        "%pip install transformers>=4.40.0 torch>=2.0.0 accelerate scikit-learn pandas numpy torch-lr-finder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "imports-setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dc7bbe-58a3-4909-e548-3419994953af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# 2. Imports and Setup Cell\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    MambaConfig,\n",
        "    MambaForCausalLM,\n",
        "    MambaModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding, # Added this import\n",
        "    TrainerCallback\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "except ImportError:\n",
        "    print(\"Not in Colab, skipping drive mount.\")\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "data-loading"
      },
      "outputs": [],
      "source": [
        "# 3. Data Loading Cell\n",
        "def load_multilingual_data(data_dir, languages=None, split='train'):\n",
        "    \"\"\"\n",
        "    Load data from multiple language files\n",
        "\n",
        "    Args:\n",
        "        data_dir: Path to directory (e.g., '/content/gdrive/MyDrive/subtask1/train')\n",
        "        languages: List of language codes (e.g., ['eng', 'arb', 'deu']) or None for all\n",
        "        split: 'train' or 'dev'\n",
        "\n",
        "    Returns:\n",
        "        combined_df: Combined DataFrame with all languages\n",
        "        language_counts: Dict with counts per language\n",
        "    \"\"\"\n",
        "    import glob\n",
        "    import os\n",
        "    import pandas as pd\n",
        "\n",
        "    # Language mapping (expanded to include all languages from both versions)\n",
        "    lang_map = {\n",
        "        'amh': 'Amharic',\n",
        "        'arb': 'Arabic',\n",
        "        'bul': 'Bulgarian',\n",
        "        'deu': 'German',\n",
        "        'eng': 'English',\n",
        "        'spa': 'Spanish',\n",
        "        'fra': 'French',\n",
        "        'hau': 'Hausa',\n",
        "        'hin': 'Hindi',\n",
        "        'ita': 'Italian',\n",
        "        'por': 'Portuguese',\n",
        "        'urd': 'Urdu',\n",
        "        'zho': 'Chinese'\n",
        "    }\n",
        "\n",
        "    all_data = []\n",
        "    language_counts = {}\n",
        "\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"LOADING {split.upper()} DATA - MULTILINGUAL\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Data directory: {data_dir}\")\n",
        "    print()\n",
        "\n",
        "    # If no languages specified, search for all available files\n",
        "    if languages is None:\n",
        "        # Try both naming patterns\n",
        "        pattern1 = os.path.join(data_dir, f'subtask1_*_{split}.csv')\n",
        "        pattern2 = os.path.join(data_dir, '*.csv')\n",
        "\n",
        "        files = glob.glob(pattern1)\n",
        "        if not files:\n",
        "            files = glob.glob(pattern2)\n",
        "            print(f\"Using pattern: *.csv\")\n",
        "        else:\n",
        "            print(f\"Using pattern: subtask1_*_{split}.csv\")\n",
        "    else:\n",
        "        files = []\n",
        "        # Try multiple naming conventions\n",
        "        for lang in languages:\n",
        "            possible_names = [\n",
        "                os.path.join(data_dir, f'subtask1_{lang}_{split}.csv'),\n",
        "                os.path.join(data_dir, f'{lang}.csv')\n",
        "            ]\n",
        "            for path in possible_names:\n",
        "                if os.path.exists(path):\n",
        "                    files.append(path)\n",
        "                    break\n",
        "\n",
        "    print(f\"Found {len(files)} files for {split} split\\\\n\")\n",
        "\n",
        "    for file_path in files:\n",
        "        filename = os.path.basename(file_path)\n",
        "\n",
        "        # Extract language code - try multiple patterns\n",
        "        try:\n",
        "            if filename.startswith('subtask1_'):\n",
        "                # Pattern: subtask1_{lang}_{split}.csv\n",
        "                lang_code = filename.split('_')[1]\n",
        "            else:\n",
        "                # Pattern: {lang}.csv\n",
        "                lang_code = filename.split('.')[0]\n",
        "        except IndexError:\n",
        "            print(f\"⚠️  Skipping file with unexpected name: {filename}\")\n",
        "            continue\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"⚠️  Warning: File not found: {file_path}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['language'] = lang_code\n",
        "        df['language_name'] = lang_map.get(lang_code, lang_code)\n",
        "\n",
        "        all_data.append(df)\n",
        "        language_counts[lang_code] = len(df)\n",
        "\n",
        "        lang_name = lang_map.get(lang_code, lang_code)\n",
        "        print(f\"✓ Loaded {lang_code} ({lang_name:12s}): {len(df):5d} samples from {filename}\")\n",
        "\n",
        "    if not all_data:\n",
        "        print(f\"\\\\n⚠️  ERROR: No data loaded. Check path: {data_dir}\")\n",
        "        print(f\"    Make sure files exist and match expected naming patterns:\")\n",
        "        print(f\"    - Pattern 1: subtask1_{{lang}}_{{split}}.csv\")\n",
        "        print(f\"    - Pattern 2: {{lang}}.csv\")\n",
        "        return pd.DataFrame(columns=['text', 'label', 'language', 'language_name']), {}\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    print(f\"\\\\n{'='*70}\")\n",
        "    print(f\"TOTAL: {len(combined_df)} samples across {len(language_counts)} languages\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Check for both 'label' and 'polarization' columns\n",
        "    if 'label' in combined_df.columns:\n",
        "        print(f\"\\\\nLabel distribution:\\\\n{combined_df['label'].value_counts(normalize=True)}\")\n",
        "    elif 'polarization' in combined_df.columns:\n",
        "        print(\"\\\\nClass Distribution:\")\n",
        "        for lang_code, count in language_counts.items():\n",
        "            lang_df = combined_df[combined_df['language'] == lang_code]\n",
        "            polarized = (lang_df['polarization'] == 1).sum()\n",
        "            non_polarized = (lang_df['polarization'] == 0).sum()\n",
        "            print(f\"  {lang_code}: Polarized={polarized}, Non-Polarized={non_polarized}\")\n",
        "    else:\n",
        "        print(\"\\\\n⚠️  Warning: No 'label' or 'polarization' column found in loaded data.\")\n",
        "\n",
        "    return combined_df, language_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mamba-model"
      },
      "outputs": [],
      "source": [
        "# 4. Mamba Model for Sequence Classification Cell\n",
        "class MambaForSequenceClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    Mamba model adapted for sequence classification\n",
        "    Uses the efficient state-space architecture instead of attention\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"state-spaces/mamba-370m-hf\", num_labels=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Load Mamba backbone\n",
        "        print(f\"Loading Mamba model: {model_name}\")\n",
        "        self.mamba = MambaModel.from_pretrained(model_name)\n",
        "\n",
        "        # Get hidden size from config\n",
        "        self.hidden_size = self.mamba.config.hidden_size\n",
        "\n",
        "        # Classification head\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.hidden_size, num_labels)\n",
        "\n",
        "        # Initialize classifier weights\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        if self.classifier.bias is not None:\n",
        "            nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass for classification\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len] (optional, may not be used by Mamba)\n",
        "            labels: Ground truth labels [batch_size] (optional)\n",
        "\n",
        "        Returns:\n",
        "            SequenceClassifierOutput with loss and logits\n",
        "        \"\"\"\n",
        "        # Get Mamba outputs\n",
        "        # Note: Mamba processes sequences differently than transformers\n",
        "        # It doesn't use attention_mask the same way, but we pass it for compatibility\n",
        "        outputs = self.mamba(input_ids=input_ids)\n",
        "\n",
        "        # Get last hidden state\n",
        "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Use mean pooling over sequence length (alternative to [CLS] token)\n",
        "        # This is more appropriate for Mamba which doesn't have special tokens\n",
        "        if attention_mask is not None:\n",
        "            # Mask out padding tokens\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "            sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "            pooled_output = sum_hidden / sum_mask\n",
        "        else:\n",
        "            # Simple mean pooling\n",
        "            pooled_output = hidden_states.mean(dim=1)\n",
        "\n",
        "        # Apply dropout and classification layer\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=None,\n",
        "            attentions=None\n",
        "        )\n",
        "\n",
        "    def freeze_backbone(self, freeze=True):\n",
        "        \"\"\"Freeze/unfreeze the Mamba backbone\"\"\"\n",
        "        for param in self.mamba.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "        print(f\"Mamba backbone {'frozen' if freeze else 'unfrozen'}\")\n",
        "\n",
        "    def unfreeze_last_n_layers(self, n=4):\n",
        "        \"\"\"Unfreeze last n layers of Mamba for fine-tuning\"\"\"\n",
        "        # Freeze all first\n",
        "        self.freeze_backbone(freeze=True)\n",
        "\n",
        "        # Unfreeze last n layers\n",
        "        total_layers = len(self.mamba.layers)\n",
        "        layers_to_unfreeze = list(range(max(0, total_layers - n), total_layers))\n",
        "\n",
        "        for idx in layers_to_unfreeze:\n",
        "            for param in self.mamba.layers[idx].parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        print(f\"Unfroze last {n} layers of Mamba (layers {layers_to_unfreeze}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dataset-class"
      },
      "outputs": [],
      "source": [
        "# 5. Dataset Class Cell\n",
        "class MultilingualDataset(Dataset):\n",
        "    \"\"\"Dataset for multilingual text classification with Mamba\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "focal-loss"
      },
      "outputs": [],
      "source": [
        "# 6. Focal Loss Cell\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for handling class imbalance\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "custom-trainer"
      },
      "outputs": [],
      "source": [
        "# 7. Custom Trainer Cell\n",
        "class FocalLossTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer that uses Focal Loss\"\"\"\n",
        "    def __init__(self, *args, use_focal_loss=False, focal_alpha=0.25, focal_gamma=2.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.use_focal_loss = use_focal_loss\n",
        "        self.focal_loss = None\n",
        "        if use_focal_loss:\n",
        "            self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): # Added num_items_in_batch\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.use_focal_loss and self.focal_loss is not None:\n",
        "            loss = self.focal_loss(logits, labels)\n",
        "        else:\n",
        "            # Default to model's internal loss (if labels were passed)\n",
        "            if outputs.loss is not None:\n",
        "                loss = outputs.loss\n",
        "            else:\n",
        "                # Or recompute CE loss if model didn't return it\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "metrics-compute"
      },
      "outputs": [],
      "source": [
        "# 8. Metrics Computation Cell\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute F1, accuracy metrics\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    f1_binary = f1_score(labels, predictions, average='binary')\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_binary': f1_binary,\n",
        "        'accuracy': accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "training-setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3b65a2-e3a9-480a-e8a5-6822329650bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "======================================================================\n",
            "LOADING TRAIN DATA - MULTILINGUAL\n",
            "======================================================================\n",
            "Data directory: /content/gdrive/MyDrive/subtask1/train\n",
            "\n",
            "Using pattern: *.csv\n",
            "Found 13 files for train split\\n\n",
            "✓ Loaded eng (English     ):  2676 samples from eng.csv\n",
            "✓ Loaded urd (Urdu        ):  2849 samples from urd.csv\n",
            "✓ Loaded arb (Arabic      ):  3380 samples from arb.csv\n",
            "✓ Loaded hau (Hausa       ):  3651 samples from hau.csv\n",
            "✓ Loaded ita (Italian     ):  3334 samples from ita.csv\n",
            "✓ Loaded nep (nep         ):  2005 samples from nep.csv\n",
            "✓ Loaded deu (German      ):  3180 samples from deu.csv\n",
            "✓ Loaded fas (fas         ):  3295 samples from fas.csv\n",
            "✓ Loaded zho (Chinese     ):  4280 samples from zho.csv\n",
            "✓ Loaded amh (Amharic     ):  3332 samples from amh.csv\n",
            "✓ Loaded tur (tur         ):  2364 samples from tur.csv\n",
            "✓ Loaded hin (Hindi       ):  2744 samples from hin.csv\n",
            "✓ Loaded spa (Spanish     ):  3305 samples from spa.csv\n",
            "\\n======================================================================\n",
            "TOTAL: 40395 samples across 13 languages\n",
            "======================================================================\n",
            "\\nClass Distribution:\n",
            "  eng: Polarized=1002, Non-Polarized=1674\n",
            "  urd: Polarized=1976, Non-Polarized=873\n",
            "  arb: Polarized=1512, Non-Polarized=1868\n",
            "  hau: Polarized=392, Non-Polarized=3259\n",
            "  ita: Polarized=1368, Non-Polarized=1966\n",
            "  nep: Polarized=1008, Non-Polarized=997\n",
            "  deu: Polarized=1512, Non-Polarized=1668\n",
            "  fas: Polarized=2440, Non-Polarized=855\n",
            "  zho: Polarized=2121, Non-Polarized=2159\n",
            "  amh: Polarized=2518, Non-Polarized=814\n",
            "  tur: Polarized=1155, Non-Polarized=1209\n",
            "  hin: Polarized=2346, Non-Polarized=398\n",
            "  spa: Polarized=1660, Non-Polarized=1645\n",
            "\n",
            "Loading validation data...\n",
            "======================================================================\n",
            "LOADING DEV DATA - MULTILINGUAL\n",
            "======================================================================\n",
            "Data directory: /content/gdrive/MyDrive/subtask1/dev\n",
            "\n",
            "Using pattern: *.csv\n",
            "Found 13 files for dev split\\n\n",
            "✓ Loaded arb (Arabic      ):   169 samples from arb.csv\n",
            "✓ Loaded hin (Hindi       ):   137 samples from hin.csv\n",
            "✓ Loaded zho (Chinese     ):   214 samples from zho.csv\n",
            "✓ Loaded urd (Urdu        ):   142 samples from urd.csv\n",
            "✓ Loaded eng (English     ):   133 samples from eng.csv\n",
            "✓ Loaded fas (fas         ):   164 samples from fas.csv\n",
            "✓ Loaded tur (tur         ):   115 samples from tur.csv\n",
            "✓ Loaded deu (German      ):   159 samples from deu.csv\n",
            "✓ Loaded spa (Spanish     ):   165 samples from spa.csv\n",
            "✓ Loaded amh (Amharic     ):   166 samples from amh.csv\n",
            "✓ Loaded ita (Italian     ):   166 samples from ita.csv\n",
            "✓ Loaded nep (nep         ):   100 samples from nep.csv\n",
            "✓ Loaded hau (Hausa       ):   182 samples from hau.csv\n",
            "\\n======================================================================\n",
            "TOTAL: 2012 samples across 13 languages\n",
            "======================================================================\n",
            "\\nClass Distribution:\n",
            "  arb: Polarized=0, Non-Polarized=0\n",
            "  hin: Polarized=0, Non-Polarized=0\n",
            "  zho: Polarized=0, Non-Polarized=0\n",
            "  urd: Polarized=0, Non-Polarized=0\n",
            "  eng: Polarized=0, Non-Polarized=0\n",
            "  fas: Polarized=0, Non-Polarized=0\n",
            "  tur: Polarized=0, Non-Polarized=0\n",
            "  deu: Polarized=0, Non-Polarized=0\n",
            "  spa: Polarized=0, Non-Polarized=0\n",
            "  amh: Polarized=0, Non-Polarized=0\n",
            "  ita: Polarized=0, Non-Polarized=0\n",
            "  nep: Polarized=0, Non-Polarized=0\n",
            "  hau: Polarized=0, Non-Polarized=0\n",
            "\n",
            "Loading tokenizer for state-spaces/mamba-370m-hf...\n",
            "\n",
            "Creating datasets...\n",
            "Train samples: 40395\n",
            "Val samples: 2012\n",
            "\n",
            "Initializing Mamba model...\n",
            "Loading Mamba model: state-spaces/mamba-370m-hf\n",
            "Mamba backbone frozen\n",
            "Unfroze last 4 layers of Mamba (layers [44, 45, 46, 47]\n",
            "Model loaded on cuda\n",
            "Total parameters: 371,518,466\n",
            "Trainable parameters: 26,671,106\n"
          ]
        }
      ],
      "source": [
        "# 9. Training Setup Cell\n",
        "torch.cuda.empty_cache()\n",
        "# Configuration\n",
        "MODEL_NAME = \"state-spaces/mamba-370m-hf\"  # 370M params, fits in 6-8GB VRAM\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 4  # Adjusted batch size\n",
        "GRADIENT_ACCUMULATION_STEPS = 8  # Adjusted to maintain effective batch size (4 * 8 = 32)\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 5\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "# Paths (adjust to your setup)\n",
        "# Ensure this path is correct in your Google Drive\n",
        "BASE_DIR = '/content/gdrive/MyDrive/subtask1'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
        "DEV_DIR = os.path.join(BASE_DIR, 'dev')\n",
        "OUTPUT_DIR = './mamba_multilingual_output'\n",
        "\n",
        "# Create output dir\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "print(\"Loading training data...\")\n",
        "train_df, train_counts = load_multilingual_data(TRAIN_DIR, split='train')\n",
        "print(\"\\nLoading validation data...\")\n",
        "val_df, val_counts = load_multilingual_data(DEV_DIR, split='dev')\n",
        "\n",
        "# Check if data loaded\n",
        "if train_df.empty or val_df.empty:\n",
        "    print(\"ERROR: Data not loaded. Check paths:\")\n",
        "    print(f\"TRAIN_DIR: {TRAIN_DIR}\")\n",
        "    print(f\"DEV_DIR: {DEV_DIR}\")\n",
        "    # Stop execution if data is missing\n",
        "    raise FileNotFoundError(\"Could not load training or validation data.\")\n",
        "\n",
        "# Initialize tokenizer (Mamba uses GPT-NeoX tokenizer)\n",
        "print(f\"\\nLoading tokenizer for {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set padding token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Tokenizer pad_token set to eos_token\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "train_dataset = MultilingualDataset(\n",
        "    texts=train_df['text'].values,\n",
        "    labels=train_df['polarization'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "val_dataset = MultilingualDataset(\n",
        "    texts=val_df['text'].values,\n",
        "    labels=val_df['polarization'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# Initialize model\n",
        "print(f\"\\nInitializing Mamba model...\")\n",
        "model = MambaForSequenceClassification(\n",
        "    model_name=MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Optional: Freeze backbone and only train last layers + classifier\n",
        "# Uncomment if you want faster training with less memory\n",
        "model.unfreeze_last_n_layers(n=4)\n",
        "\n",
        "# Move to GPU if available\n",
        "# device is defined in the import cell\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "training-run",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "c9daafe1-8c1b-4670-a424-8af126348bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING TRAINING\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-252344041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STARTING TRAINING\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Save final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 10. Training Arguments and Training Cell\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_dir=f'{OUTPUT_DIR}/logs',\n",
        "    logging_steps=10, # Changed from 50 to 10 for more verbose output\n",
        "    eval_steps=150,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=150,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    dataloader_num_workers=0, # Changed from 2 to 0 to prevent potential hanging issues\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=False,  # Add this line - explicitly enable progress bar\n",
        "    logging_first_step=True,  # Add this line - shows progress immediately\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = FocalLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    use_focal_loss=False,  # Set to True if you want focal loss\n",
        "    focal_alpha=0.25,\n",
        "    focal_gamma=2.0,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "trainer.train()\n",
        "\n",
        "# Save final model\n",
        "print(\"\\nSaving final model...\")\n",
        "final_model_path = os.path.join(OUTPUT_DIR, 'final_model')\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(f\"Model saved to {final_model_path}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "threshold-find"
      },
      "outputs": [],
      "source": [
        "# 11. Threshold Finding Cell\n",
        "\n",
        "def find_optimal_threshold(model, dataset, device, metric='f1_macro'):\n",
        "    \"\"\"\n",
        "    Find optimal classification threshold for binary classification\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"FINDING OPTIMAL THRESHOLD FOR {metric.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    from torch.utils.data import DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    print(f\"Validation samples: {len(dataset)}\")\n",
        "    print(\"Generating predictions...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probs = F.softmax(outputs.logits, dim=-1)[:, 1]  # Probability of class 1\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Test different thresholds\n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "    best_threshold = 0.5\n",
        "    best_score = 0\n",
        "\n",
        "    print(f\"Testing {len(thresholds)} thresholds...\")\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        preds = (all_probs >= threshold).astype(int)\n",
        "\n",
        "        if metric == 'f1_macro':\n",
        "            score = f1_score(all_labels, preds, average='macro')\n",
        "        elif metric == 'f1_binary':\n",
        "            score = f1_score(all_labels, preds, average='binary')\n",
        "        else:\n",
        "            score = accuracy_score(all_labels, preds)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_threshold = threshold\n",
        "\n",
        "    print(f\"\\nOptimal threshold: {best_threshold:.3f}\")\n",
        "    print(f\"Best {metric}: {best_score:.4f}\")\n",
        "\n",
        "    # Final predictions with optimal threshold\n",
        "    final_preds = (all_probs >= best_threshold).astype(int)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, final_preds, digits=4))\n",
        "\n",
        "    return best_threshold, best_score\n",
        "\n",
        "# Find optimal thresholds\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINDING OPTIMAL THRESHOLD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "threshold_f1_macro = 0.5\n",
        "score_f1_macro = 0.0\n",
        "threshold_f1_binary = 0.5\n",
        "score_f1_binary = 0.0\n",
        "\n",
        "# Ensure val_dataset and device are available from the previous cell\n",
        "if 'val_dataset' in globals() and 'device' in globals() and 'model' in globals():\n",
        "    threshold_f1_macro, score_f1_macro = find_optimal_threshold(\n",
        "        model, val_dataset, device, metric='f1_macro'\n",
        "    )\n",
        "    threshold_f1_binary, score_f1_binary = find_optimal_threshold(\n",
        "        model, val_dataset, device, metric='f1_binary'\n",
        "    )\n",
        "\n",
        "    # Save thresholds\n",
        "    thresholds = {\n",
        "        'f1_macro': {'threshold': float(threshold_f1_macro), 'score': float(score_f1_macro)},\n",
        "        'f1_binary': {'threshold': float(threshold_f1_binary), 'score': float(score_f1_binary)}\n",
        "    }\n",
        "\n",
        "    threshold_path = os.path.join(OUTPUT_DIR, 'optimal_thresholds.json')\n",
        "    with open(threshold_path, 'w') as f:\n",
        "        json.dump(thresholds, f, indent=2)\n",
        "\n",
        "    print(f\"\\nThresholds saved to {threshold_path}\")\n",
        "else:\n",
        "    print(\"Could not find val_dataset or device. Skipping threshold finding.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference-func"
      },
      "outputs": [],
      "source": [
        "# 12. Inference Function Cell\n",
        "\n",
        "def predict_with_mamba(texts, model, tokenizer, device, threshold=0.5, batch_size=16):\n",
        "    \"\"\"\n",
        "    Make predictions on new texts using Mamba model\n",
        "\n",
        "    Args:\n",
        "        texts: List of text strings or a single text string\n",
        "        model: Trained MambaForSequenceClassification model\n",
        "        tokenizer: Tokenizer\n",
        "        device: torch device\n",
        "        threshold: Classification threshold\n",
        "        batch_size: Batch size for inference\n",
        "\n",
        "    Returns:\n",
        "        predictions: List of predicted labels (0 or 1)\n",
        "        probabilities: List of probabilities for class 1\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "    # Handle single text input\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    # Tokenize all texts\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probs = F.softmax(outputs.logits, dim=-1)[:, 1]\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    predictions = (all_probs >= threshold).astype(int)\n",
        "\n",
        "    return predictions.tolist(), all_probs.tolist()\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INFERENCE EXAMPLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_texts = [\n",
        "    \"This is a great product! I love it!\",\n",
        "    \"Terrible experience, would not recommend.\"\n",
        "]\n",
        "\n",
        "# Ensure model, tokenizer, device, and threshold are available\n",
        "if 'model' in globals() and 'tokenizer' in globals() and 'device' in globals() and 'threshold_f1_macro' in globals():\n",
        "    predictions, probabilities = predict_with_mamba(\n",
        "        test_texts, model, tokenizer, device, threshold=threshold_f1_macro\n",
        "    )\n",
        "\n",
        "    for text, pred, prob in zip(test_texts, predictions, probabilities):\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Prediction: {pred} (prob: {prob:.4f})\\n\")\n",
        "else:\n",
        "    print(\"Could not run example: model, tokenizer, or threshold not found.\")\n",
        "    print(\"Please run the training cells first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}