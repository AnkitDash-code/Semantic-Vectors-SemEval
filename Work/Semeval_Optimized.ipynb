{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "Ls1UPn_tlIK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXapiv1YhQwN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.40.0 torch>=2.0.0 accelerate scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # âœ… Added for Focal Loss\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split  # âœ… Added for proper split\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii-AjKfOhtRg",
        "outputId": "cb006ade-5533-46d2-ca25-78ea0e42f907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BitLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    1.58-bit Quantized Linear Layer (BitNet)\n",
        "\n",
        "    Key Features:\n",
        "    - Weights: Ternary quantization {-1, 0, +1}\n",
        "    - Activations: 8-bit quantization [-128, 127]\n",
        "    - Straight-Through Estimator (STE) for gradient flow\n",
        "    - Lambda warmup for gradual quantization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Initialize weights with Xavier uniform (better for deep networks)\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Layer normalization before quantization (critical for stability)\n",
        "        self.layer_norm = nn.LayerNorm(in_features)\n",
        "\n",
        "        # Lambda for gradual quantization warmup (starts at 0, goes to 1)\n",
        "        self.register_buffer('lambda_val', torch.tensor(0.0))\n",
        "\n",
        "    def weight_quant(self, w):\n",
        "        \"\"\"\n",
        "        Quantize weights to ternary values {-1, 0, +1}\n",
        "        Uses round-to-nearest with scale normalization\n",
        "        \"\"\"\n",
        "        # Calculate scale factor using mean absolute value\n",
        "        scale = 1.0 / w.abs().mean().clamp_(min=1e-5)\n",
        "        # Round to nearest integer and clamp to [-1, 0, 1]\n",
        "        w_quant = (w * scale).round().clamp_(-1, 1) / scale\n",
        "        return w_quant\n",
        "\n",
        "    def activation_quant(self, x):\n",
        "        \"\"\"\n",
        "        Quantize activations to 8-bit using absmax quantization\n",
        "        Maps to [-128, 127] range\n",
        "        \"\"\"\n",
        "        # Find maximum absolute value per sample\n",
        "        scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n",
        "        # Quantize and dequantize\n",
        "        x_quant = (x * scale).round().clamp_(-128, 127) / scale\n",
        "        return x_quant\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply layer normalization first\n",
        "        x_norm = self.layer_norm(x)\n",
        "\n",
        "        # Get current lambda value (controls quantization strength)\n",
        "        lambda_val = self.lambda_val.item()\n",
        "\n",
        "        if self.training:\n",
        "            # During training: gradual quantization with lambda warmup\n",
        "            x_quant_full = self.activation_quant(x_norm)\n",
        "            w_quant_full = self.weight_quant(self.weight)\n",
        "\n",
        "            # Linear interpolation between full precision and quantized\n",
        "            # Lambda = 0: full precision, Lambda = 1: full quantization\n",
        "            x_mixed = x_norm * (1 - lambda_val) + x_quant_full * lambda_val\n",
        "            w_mixed = self.weight * (1 - lambda_val) + w_quant_full * lambda_val\n",
        "\n",
        "            # Straight-Through Estimator: forward with quantized, backward with original\n",
        "            x_final = x_mixed + (x_quant_full - x_mixed).detach()\n",
        "            w_final = w_mixed + (w_quant_full - w_mixed).detach()\n",
        "        else:\n",
        "            # During inference: full quantization (lambda = 1)\n",
        "            x_final = x_norm + (self.activation_quant(x_norm) - x_norm).detach()\n",
        "            w_final = self.weight + (self.weight_quant(self.weight) - self.weight).detach()\n",
        "\n",
        "        # Standard linear transformation\n",
        "        return F.linear(x_final, w_final, self.bias)"
      ],
      "metadata": {
        "id": "Y7DDaUCKiQHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BitNetBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    BitNet model for binary polarization detection\n",
        "    Architecture: BERT -> BitLinear Layers -> Classification\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained BERT\n",
        "        print(f\"Loading BERT model: {model_name}\")\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        config = self.bert.config\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Optional: Freeze early BERT layers for efficiency\n",
        "        # Uncomment to freeze first 8 layers (keeps last 4 trainable)\n",
        "        # for layer in self.bert.encoder.layer[:8]:\n",
        "        #     for param in layer.parameters():\n",
        "        #         param.requires_grad = False\n",
        "\n",
        "        # BitLinear classification head (2 layers for better representation)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.bit_fc1 = BitLinear(config.hidden_size, config.hidden_size // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.bit_fc2 = BitLinear(config.hidden_size // 2, num_labels)\n",
        "\n",
        "        print(f\"Model initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # Get BERT embeddings\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # Use [CLS] token representation (pooled output)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # Pass through BitLinear layers\n",
        "        x = self.bit_fc1(pooled_output)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.bit_fc2(x)\n",
        "\n",
        "        # Compute loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        # Return in HuggingFace format\n",
        "        return SequenceClassifierOutput(\n",
        "              loss=loss,\n",
        "              logits=logits,\n",
        "              hidden_states=None,\n",
        "              attentions=None)"
      ],
      "metadata": {
        "id": "REJvh3v8iZqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolarizationDataset(Dataset):\n",
        "    \"\"\"Dataset class for polarization detection\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize with proper truncation\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=False,  # Handled by DataCollator\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Squeeze to remove batch dimension\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "p53peM3Hiier"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for handling class imbalance\n",
        "    Reference: https://arxiv.org/abs/1708.02002\n",
        "\n",
        "    Better than weighted CE for imbalanced classification because it:\n",
        "    - Focuses on hard-to-classify examples\n",
        "    - Down-weights easy examples\n",
        "    - Reduces false positives\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.65, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "\n",
        "class BitNetTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom trainer with:\n",
        "    - Gradual quantization warmup (lambda scheduling)\n",
        "    - Option for Weighted CE or Focal Loss for class imbalance\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup_steps=1000, class_weight=None, use_focal_loss=False, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.use_focal_loss = use_focal_loss\n",
        "\n",
        "        # Handle class weights\n",
        "        if class_weight is not None:\n",
        "            self.class_weight = class_weight.to(self.args.device)\n",
        "        else:\n",
        "            self.class_weight = None\n",
        "\n",
        "        print(f\"Lambda warmup enabled: 0 -> 1 over {warmup_steps} steps\")\n",
        "\n",
        "        # Initialize loss function\n",
        "        if self.use_focal_loss:\n",
        "            self.focal_loss = FocalLoss(alpha=0.65, gamma=2.0)\n",
        "            print(f\"Using Focal Loss (alpha=0.65, gamma=2.0)\")\n",
        "        elif self.class_weight is not None:\n",
        "            print(f\"Using Weighted CE Loss with weights: {self.class_weight}\")\n",
        "        else:\n",
        "            print(f\"Using standard Cross-Entropy Loss\")\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Calculate lambda based on current training step\n",
        "        current_step = self.state.global_step\n",
        "        lambda_val = min(1.0, current_step / self.warmup_steps)\n",
        "\n",
        "        # Set lambda for all BitLinear layers\n",
        "        for module in model.modules():\n",
        "            if hasattr(module, 'lambda_val'):\n",
        "                module.lambda_val.fill_(lambda_val)\n",
        "\n",
        "        # Get labels and perform forward pass\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss based on selected method\n",
        "        if self.use_focal_loss:\n",
        "            loss = self.focal_loss(logits, labels)\n",
        "        elif self.class_weight is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weight)\n",
        "            loss = loss_fct(logits, labels)\n",
        "        else:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "3yCEKOGFinO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for binary classification\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return {\n",
        "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "        'f1_binary': f1_score(labels, preds, average='binary'),\n",
        "        'accuracy': accuracy_score(labels, preds)\n",
        "    }"
      ],
      "metadata": {
        "id": "VKnCV3l3ir-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWiAQNeoFDUs"
      },
      "source": [
        "def train_polarization_detector():\n",
        "    \"\"\"\n",
        "    OPTIMIZED training function for maximizing F1 Macro\n",
        "\n",
        "    Key optimizations:\n",
        "    - Proper 80/20 train/val split with stratification\n",
        "    - Focal Loss for better class imbalance handling\n",
        "    - Increased dropout (0.2) and weight decay (0.02)\n",
        "    - 10 epochs with early stopping\n",
        "    - Optimized learning rate (2e-5)\n",
        "    - FP16 mixed precision for faster training\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"SUBTASK 1: BINARY POLARIZATION DETECTION\")\n",
        "    print(\"1-Bit LLM with BitLinear Quantization - OPTIMIZED\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load and split data\n",
        "    print(\"\\nLoading data...\")\n",
        "    train_full = pd.read_csv('train_eng.csv')\n",
        "\n",
        "    # Proper 80/20 split with stratification\n",
        "    train, val = train_test_split(\n",
        "        train_full,\n",
        "        test_size=0.2,\n",
        "        stratify=train_full['polarization'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train)}\")\n",
        "    print(f\"Validation samples: {len(val)}\")\n",
        "    print(f\"\\nTraining label distribution:\")\n",
        "    print(train['polarization'].value_counts())\n",
        "    print(f\"\\nValidation label distribution:\")\n",
        "    print(val['polarization'].value_counts())\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    model_name = 'bert-base-uncased'\n",
        "    print(f\"\\nInitializing tokenizer: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = PolarizationDataset(\n",
        "        train['text'].tolist(),\n",
        "        train['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    val_dataset = PolarizationDataset(\n",
        "        val['text'].tolist(),\n",
        "        val['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Initialize model with optimized dropout\n",
        "    print(\"\\nInitializing BitNet model...\")\n",
        "    model = BitNetBinaryClassifier(\n",
        "        model_name=model_name,\n",
        "        num_labels=2,\n",
        "        dropout_prob=0.2  # âœ… Increased from 0.1 for better generalization\n",
        "    )\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = torch.tensor([0.82, 1.30], dtype=torch.float32)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    # OPTIMIZED Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=10,  # âœ… Increased from 7 for better convergence\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_steps=500,\n",
        "        learning_rate=2e-5,  # âœ… Optimized learning rate\n",
        "        weight_decay=0.02,  # âœ… Increased regularization\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        eval_strategy='steps',\n",
        "        eval_steps=100,\n",
        "        save_strategy='steps',\n",
        "        save_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1_macro',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        report_to='none',\n",
        "        seed=42,\n",
        "        fp16=torch.cuda.is_available(),  # âœ… FP16 only if CUDA available\n",
        "    )\n",
        "\n",
        "    # Initialize custom trainer with FOCAL LOSS\n",
        "    trainer = BitNetTrainer(\n",
        "        warmup_steps=1000,\n",
        "        class_weight=class_weights,\n",
        "        use_focal_loss=True,  # âœ… Using Focal Loss for better F1 Macro\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STARTING OPTIMIZED TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "    results = trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION ON VALIDATION SET\")\n",
        "    print(\"=\"*60)\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"\\nValidation Metrics:\")\n",
        "    for key, value in eval_results.items():\n",
        "        if isinstance(value, (int, float)):\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "    # âœ… Proper way to save PyTorch model + HuggingFace tokenizer\n",
        "    import os\n",
        "    save_dir = './bitnet_polarization_model_optimized'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "      # Save model state dict (PyTorch standard)\n",
        "    torch.save({\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'model_config': {\n",
        "              'model_name': 'bert-base-uncased',\n",
        "              'num_labels': 2,\n",
        "              'dropout_prob': 0.2\n",
        "          }\n",
        "      }, os.path.join(save_dir, 'pytorch_model.bin'))\n",
        "\n",
        "      # Save tokenizer (HuggingFace - this method works for tokenizer!)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "\n",
        "    return model, tokenizer, trainer, eval_results\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_polarization(text, model, tokenizer, return_probabilities=True):\n",
        "    \"\"\"\n",
        "    Make predictions on new text\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "        model: Trained BitNet model\n",
        "        tokenizer: BERT tokenizer\n",
        "        return_probabilities: If True, return probabilities along with prediction\n",
        "\n",
        "    Returns:\n",
        "        prediction: 0 (Not Polarized) or 1 (Polarized)\n",
        "        confidence: Probability of being polarized (if return_probabilities=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        pred = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][1].item()  # Probability of being polarized\n",
        "\n",
        "    if return_probabilities:\n",
        "        return pred, confidence\n",
        "    return pred"
      ],
      "metadata": {
        "id": "wqx6sTaRjAgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_submission(model, tokenizer, test_file='dev_eng.csv', output_file='dev_predictions.csv', threshold=0.48):\n",
        "    \"\"\"\n",
        "    Create predictions for dev/test dataset with custom threshold\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        tokenizer: Tokenizer\n",
        "        test_file: Path to test file\n",
        "        output_file: Output CSV file\n",
        "        threshold: Decision threshold (default 0.48)\n",
        "\n",
        "    Returns:\n",
        "        submission: DataFrame with predictions\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating predictions from {test_file}...\")\n",
        "    print(f\"Using threshold: {threshold}\")\n",
        "\n",
        "    # Load test data\n",
        "    test = pd.read_csv(test_file)\n",
        "    print(f\"Test samples: {len(test)}\")\n",
        "\n",
        "    # Create dataset with dummy labels\n",
        "    test_dataset = PolarizationDataset(\n",
        "        test['text'].tolist(),\n",
        "        [0] * len(test),  # Dummy labels\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Create trainer for prediction\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    print(\"Generating predictions...\")\n",
        "    prediction_output = trainer.predict(test_dataset)\n",
        "    raw_predictions = prediction_output.predictions\n",
        "\n",
        "    # Convert to tensor and handle shape\n",
        "    if not isinstance(raw_predictions, torch.Tensor):\n",
        "        raw_predictions = torch.tensor(raw_predictions)\n",
        "\n",
        "    # Ensure correct shape (num_samples, num_labels)\n",
        "    if raw_predictions.dim() == 1:\n",
        "        # If 1D, reshape to (num_samples, num_labels)\n",
        "        raw_predictions = raw_predictions.reshape(-1, 2)\n",
        "    elif raw_predictions.shape[1] != 2:\n",
        "        # If shape is unexpected, try to fix it\n",
        "        raw_predictions = raw_predictions.reshape(len(test), -1)[:, :2]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = F.softmax(raw_predictions, dim=1)\n",
        "    pred_probs = probs[:, 1].numpy()  # Probability of polarized class\n",
        "\n",
        "    # Apply threshold\n",
        "    pred_labels = (pred_probs >= threshold).astype(int)\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'text': test['text'],\n",
        "        'predicted_polarization': pred_labels,\n",
        "        'polarization_probability': pred_probs\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    submission.to_csv(output_file, index=False)\n",
        "    print(f\"\\nPredictions saved to: {output_file}\")\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\nPrediction Statistics:\")\n",
        "    print(f\"  Non-polarized (0): {(pred_labels == 0).sum()} ({(pred_labels == 0).sum() / len(pred_labels) * 100:.2f}%)\")\n",
        "    print(f\"  Polarized (1): {(pred_labels == 1).sum()} ({(pred_labels == 1).sum() / len(pred_labels) * 100:.2f}%)\")\n",
        "    print(f\"  Mean probability: {pred_probs.mean():.4f}\")\n",
        "    print(f\"  Median probability: {np.median(pred_probs):.4f}\")\n",
        "\n",
        "    return submission\n"
      ],
      "metadata": {
        "id": "K3Zlg5W-84VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIUR_0fFKkvu"
      },
      "source": [
        "def find_optimal_threshold(model, tokenizer, val_file='train_eng.csv'):\n",
        "    \"\"\"\n",
        "    Find optimal threshold for F1 Macro maximization\n",
        "\n",
        "    Uses the validation split from training data to find best threshold\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        tokenizer: Tokenizer\n",
        "        val_file: Training file (will use 20% validation split)\n",
        "\n",
        "    Returns:\n",
        "        best_threshold: Optimal threshold\n",
        "        best_f1_macro: Best F1 Macro achieved\n",
        "        results_df: Full results DataFrame\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINDING OPTIMAL THRESHOLD FOR F1 MACRO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load and split data same way as training\n",
        "    full_data = pd.read_csv(val_file)\n",
        "    _, val_data = train_test_split(\n",
        "        full_data,\n",
        "        test_size=0.2,\n",
        "        stratify=full_data['polarization'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "    # Create dataset\n",
        "    val_dataset = PolarizationDataset(\n",
        "        val_data['text'].tolist(),\n",
        "        val_data['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    raw_predictions = predictions.predictions\n",
        "\n",
        "    # Convert to probabilities\n",
        "    if not isinstance(raw_predictions, torch.Tensor):\n",
        "        raw_predictions = torch.tensor(raw_predictions)\n",
        "\n",
        "    if raw_predictions.dim() == 1:\n",
        "        raw_predictions = raw_predictions.reshape(-1, 2)\n",
        "\n",
        "    probs = F.softmax(raw_predictions, dim=1)[:, 1].numpy()\n",
        "    true_labels = val_data['polarization'].values\n",
        "\n",
        "    # Test thresholds\n",
        "    print(\"\\nTesting thresholds from 0.30 to 0.70...\")\n",
        "    thresholds = np.arange(0.30, 0.71, 0.01)\n",
        "\n",
        "    results = []\n",
        "    for thresh in thresholds:\n",
        "        pred_labels = (probs >= thresh).astype(int)\n",
        "\n",
        "        f1_macro = f1_score(true_labels, pred_labels, average='macro')\n",
        "        f1_binary = f1_score(true_labels, pred_labels, average='binary', zero_division=0)\n",
        "        precision = (pred_labels[pred_labels == 1] == true_labels[pred_labels == 1]).sum() / max(pred_labels.sum(), 1)\n",
        "        recall = (pred_labels[true_labels == 1] == 1).sum() / max((true_labels == 1).sum(), 1)\n",
        "        accuracy = (pred_labels == true_labels).sum() / len(true_labels)\n",
        "\n",
        "        results.append({\n",
        "            'threshold': thresh,\n",
        "            'f1_macro': f1_macro,\n",
        "            'f1_binary': f1_binary,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'accuracy': accuracy\n",
        "        })\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Find best threshold\n",
        "    best_idx = results_df['f1_macro'].idxmax()\n",
        "    best_result = results_df.loc[best_idx]\n",
        "\n",
        "    # Print top 5 thresholds\n",
        "    print(\"\\nTop 5 thresholds by F1 Macro:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Threshold':<12} {'F1 Macro':<12} {'F1 Binary':<12} {'Precision':<12} {'Recall':<12}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    top_5 = results_df.nlargest(5, 'f1_macro')\n",
        "    for _, row in top_5.iterrows():\n",
        "        print(f\"{row['threshold']:.2f}         {row['f1_macro']:.4f}       \"\n",
        "              f\"{row['f1_binary']:.4f}       {row['precision']:.4f}       {row['recall']:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OPTIMAL THRESHOLD FOUND\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best Threshold: {best_result['threshold']:.2f}\")\n",
        "    print(f\"F1 Macro: {best_result['f1_macro']:.4f}\")\n",
        "    print(f\"F1 Binary: {best_result['f1_binary']:.4f}\")\n",
        "    print(f\"Precision: {best_result['precision']:.4f}\")\n",
        "    print(f\"Recall: {best_result['recall']:.4f}\")\n",
        "    print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
        "\n",
        "    # Save full results\n",
        "    results_df.to_csv('threshold_optimization_results.csv', index=False)\n",
        "    print(\"\\nFull results saved to: threshold_optimization_results.csv\")\n",
        "\n",
        "    return best_result['threshold'], best_result['f1_macro'], results_df\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference_examples(model, tokenizer):\n",
        "    \"\"\"Test model on example texts\"\"\"\n",
        "\n",
        "    test_examples = [\n",
        "        \"This politician is destroying our country with terrible policies!\",\n",
        "        \"I believe we need better education and healthcare systems.\",\n",
        "        \"Those people are all criminals and should be deported immediately!\",\n",
        "        \"Research shows that renewable energy can reduce carbon emissions.\",\n",
        "        \"They're trying to take away our rights and freedoms!\",\n",
        "        \"The weather forecast predicts rain tomorrow afternoon.\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INFERENCE EXAMPLES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, text in enumerate(test_examples, 1):\n",
        "        pred, confidence = predict_polarization(text, model, tokenizer)\n",
        "        label = \"Polarized\" if pred == 1 else \"Not Polarized\"\n",
        "        print(f\"\\n{i}. Text: {text}\")\n",
        "        print(f\"   Prediction: {label}\")\n",
        "        print(f\"   Confidence: {confidence:.3f}\")"
      ],
      "metadata": {
        "id": "ZsjWq2yAjbSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TT5Vb4WRFDUu",
        "outputId": "8b93a580-673e-4951-d5a5-b9fb104e834b"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"OPTIMIZED WORKFLOW FOR F1 MACRO > 0.80\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # STEP 1: Train the model with optimized hyperparameters\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1/4: TRAINING MODEL\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model, tokenizer, trainer, train_results = train_polarization_detector()\n",
        "\n",
        "    # STEP 2: Find optimal threshold\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2/4: FINDING OPTIMAL THRESHOLD\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    optimal_threshold, best_f1_macro, threshold_results = find_optimal_threshold(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        val_file='train_eng.csv'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Optimal threshold: {optimal_threshold:.2f}\")\n",
        "    print(f\"ðŸŽ¯ Expected F1 Macro: {best_f1_macro:.4f}\")\n",
        "\n",
        "    # STEP 3: Test on example texts\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3/4: TESTING ON EXAMPLES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    test_inference_examples(model, tokenizer)\n",
        "\n",
        "    # STEP 4: Generate predictions for dev dataset with optimal threshold\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 4/4: GENERATING DEV PREDICTIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    submission = create_submission(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        test_file='dev_eng.csv',\n",
        "        output_file='dev_predictions_optimized.csv',\n",
        "        threshold=optimal_threshold  # âœ… Using optimal threshold\n",
        "    )\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING AND PREDICTION COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nðŸ“Š FINAL SUMMARY:\")\n",
        "    print(f\"   Model: BitNet with Focal Loss (Î±=0.65, Î³=2.0)\")\n",
        "    print(f\"   Training epochs: 10\")\n",
        "    print(f\"   Dropout: 0.2\")\n",
        "    print(f\"   Learning rate: 2e-5\")\n",
        "    print(f\"   Weight decay: 0.02\")\n",
        "    print(f\"   Optimal threshold: {optimal_threshold:.2f}\")\n",
        "    print(f\"   Validation F1 Macro: {best_f1_macro:.4f}\")\n",
        "\n",
        "    print(\"\\nðŸ“ OUTPUT FILES:\")\n",
        "    print(\"   âœ… Model: ./bitnet_polarization_model_optimized/\")\n",
        "    print(\"   âœ… Predictions: dev_predictions_optimized.csv\")\n",
        "    print(\"   âœ… Threshold analysis: threshold_optimization_results.csv\")\n",
        "\n",
        "    print(\"\\nðŸ’¡ PERFORMANCE ANALYSIS:\")\n",
        "    if best_f1_macro >= 0.80:\n",
        "        print(f\"   âœ… SUCCESS! F1 Macro = {best_f1_macro:.4f} >= 0.80\")\n",
        "        print(\"   ðŸŽ‰ Target achieved!\")\n",
        "    elif best_f1_macro >= 0.78:\n",
        "        print(f\"   âš¡ CLOSE! F1 Macro = {best_f1_macro:.4f}\")\n",
        "        print(\"   Try: Increase epochs to 12-15 or try RoBERTa-base\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸  Current F1 Macro = {best_f1_macro:.4f}\")\n",
        "        print(\"   Suggestions:\")\n",
        "        print(\"   - Try RoBERTa: model_name = 'roberta-base'\")\n",
        "        print(\"   - Train ensemble of 3-5 models\")\n",
        "        print(\"   - Tune Focal Loss: alpha=[0.6, 0.65, 0.7], gamma=[1.5, 2.0, 2.5]\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "OPTIMIZED WORKFLOW FOR F1 MACRO > 0.80\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 1/4: TRAINING MODEL\n",
            "======================================================================\n",
            "============================================================\n",
            "SUBTASK 1: BINARY POLARIZATION DETECTION\n",
            "1-Bit LLM with BitLinear Quantization - OPTIMIZED\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "Training samples: 2140\n",
            "Validation samples: 536\n",
            "\n",
            "Training label distribution:\n",
            "polarization\n",
            "0    1339\n",
            "1     801\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Validation label distribution:\n",
            "polarization\n",
            "0    335\n",
            "1    201\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Initializing tokenizer: bert-base-uncased\n",
            "Creating datasets...\n",
            "\n",
            "Initializing BitNet model...\n",
            "Loading BERT model: bert-base-uncased\n",
            "Model initialized with 109,780,610 parameters\n",
            "Total parameters: 109,780,610\n",
            "Trainable parameters: 109,780,610\n",
            "Class weights: tensor([0.8200, 1.3000])\n",
            "Lambda warmup enabled: 0 -> 1 over 1000 steps\n",
            "Using Focal Loss (alpha=0.65, gamma=2.0)\n",
            "\n",
            "============================================================\n",
            "STARTING OPTIMIZED TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='701' max='1340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 701/1340 03:42 < 03:23, 3.14 it/s, Epoch 5.22/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Binary</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.183700</td>\n",
              "      <td>0.108141</td>\n",
              "      <td>0.615971</td>\n",
              "      <td>0.527845</td>\n",
              "      <td>0.636194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.113363</td>\n",
              "      <td>0.698824</td>\n",
              "      <td>0.602740</td>\n",
              "      <td>0.729478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.107354</td>\n",
              "      <td>0.733155</td>\n",
              "      <td>0.694323</td>\n",
              "      <td>0.738806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.101000</td>\n",
              "      <td>0.106136</td>\n",
              "      <td>0.734485</td>\n",
              "      <td>0.650273</td>\n",
              "      <td>0.761194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.069100</td>\n",
              "      <td>0.135308</td>\n",
              "      <td>0.740325</td>\n",
              "      <td>0.670077</td>\n",
              "      <td>0.759328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.043300</td>\n",
              "      <td>0.148034</td>\n",
              "      <td>0.762899</td>\n",
              "      <td>0.696104</td>\n",
              "      <td>0.781716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.030200</td>\n",
              "      <td>0.163519</td>\n",
              "      <td>0.776862</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.787313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}