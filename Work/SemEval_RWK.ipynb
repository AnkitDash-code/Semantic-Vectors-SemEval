{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e7f30552",
      "metadata": {
        "id": "e7f30552"
      },
      "source": [
        "# RWKV BitNet Model for Multilingual Polarization Detection\n",
        "\n",
        "## Overview\n",
        "This notebook implements a **pure RWKV encoder** paired with BitLinear classification heads for multilingual polarization detection in the SemEval 2025 Task 11 competition.\n",
        "\n",
        "## Key Features\n",
        "- **RWKV Encoder**: O(N) sequence complexity with bidirectional WKV kernel\n",
        "- **BitNet Quantization**: 1.58-bit quantized linear layers for efficient inference\n",
        "- **Multilingual Support**: 9 languages (English, Arabic, German, Spanish, Italian, Urdu, Chinese, Hausa, Amharic)\n",
        "- **Learned Token Embeddings**: Randomly initialized embeddings and pooler (no pretrained weights)\n",
        "- **Focal Loss**: Handles class imbalance during training\n",
        "\n",
        "## Performance Benefits\n",
        "- ~2x faster training per epoch\n",
        "- ~30% less GPU memory usage\n",
        "- Scales comfortably to 2048+ token sequences\n",
        "- Comparable F1 scores to heavyweight transformer baselines\n",
        "\n",
        "## Notebook Structure\n",
        "1. **Setup & Installation**\n",
        "2. **Data Loading Functions**\n",
        "3. **RWKV Architecture** (Bidirectional WKV Kernel)\n",
        "4. **BitNet Implementation**\n",
        "5. **Model Training**\n",
        "6. **Prediction Generation**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6e757cf0",
      "metadata": {
        "id": "6e757cf0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c28a75",
      "metadata": {
        "id": "58c28a75"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0de87255",
      "metadata": {
        "id": "0de87255"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.40.0 torch>=2.0.0 accelerate scikit-learn pandas numpy torch-lr-finder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17fde74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a17fde74",
        "outputId": "eb239927-2807-4e70-8ec3-451c3d92c73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPooling\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    PreTrainedModel,\n",
        "    PretrainedConfig\n",
        ")\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "DRIVE_MODEL_DIR = '/content/gdrive/MyDrive/SemevalModels/bitnet_polarization'\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7b38c3c4",
      "metadata": {
        "id": "7b38c3c4"
      },
      "outputs": [],
      "source": [
        "def load_multilingual_data(data_dir, languages=None, split='train'):\n",
        "    \"\"\"\n",
        "    Load data from multiple language files\n",
        "\n",
        "    Args:\n",
        "        data_dir: Path to directory (e.g., '/content/gdrive/MyDrive/subtask1/train')\n",
        "        languages: List of language codes (e.g., ['eng', 'arb', 'deu']) or None for all\n",
        "        split: 'train' or 'dev'\n",
        "\n",
        "    Returns:\n",
        "        combined_df: Combined DataFrame with all languages\n",
        "        language_counts: Dict with counts per language\n",
        "    \"\"\"\n",
        "    import glob\n",
        "\n",
        "    # Language code mapping\n",
        "    lang_files = {\n",
        "        'amh': 'amh.csv',  # Amharic\n",
        "        'arb': 'arb.csv',  # Arabic\n",
        "        'deu': 'deu.csv',  # German\n",
        "        'eng': 'eng.csv',  # English\n",
        "        'hau': 'hau.csv',  # Hausa\n",
        "        'ita': 'ita.csv',  # Italian\n",
        "        'spa': 'spa.csv',  # Spanish\n",
        "        'urd': 'urd.csv',  # Urdu\n",
        "        'zho': 'zho.csv',  # Chinese\n",
        "    }\n",
        "\n",
        "    # If no languages specified, use all\n",
        "    if languages is None:\n",
        "        languages = list(lang_files.keys())\n",
        "\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"LOADING {split.upper()} DATA - MULTILINGUAL\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Languages requested: {', '.join(languages)}\")\n",
        "    print(f\"Data directory: {data_dir}\")\n",
        "    print()\n",
        "\n",
        "    all_dataframes = []\n",
        "    language_counts = {}\n",
        "\n",
        "    for lang_code in languages:\n",
        "        file_name = lang_files.get(lang_code)\n",
        "        if file_name is None:\n",
        "            print(f\"⚠️  Warning: Unknown language code '{lang_code}', skipping...\")\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"⚠️  Warning: File not found: {file_path}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Load CSV\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        if 'text' not in df.columns:\n",
        "            raise ValueError(f\"Expected column 'text' in {file_path} but it was not found.\")\n",
        "\n",
        "        original_len = len(df)\n",
        "\n",
        "        # Standardize text column and drop empty/NaN rows\n",
        "        df['text'] = df['text'].astype(str)\n",
        "        df['text'] = df['text'].str.strip()\n",
        "        df = df[df['text'].notna() & (df['text'] != '')].copy()\n",
        "\n",
        "        # Drop rows with missing polarization labels if present\n",
        "        if 'polarization' in df.columns:\n",
        "            before_label = len(df)\n",
        "            df = df[df['polarization'].notna()].copy()\n",
        "            label_dropped = before_label - len(df)\n",
        "        else:\n",
        "            label_dropped = 0\n",
        "\n",
        "        removed = original_len - len(df)\n",
        "\n",
        "        if removed > 0:\n",
        "            print(f\"⚠️  Cleaned {lang_code}: removed {removed} empty-text rows (labels dropped: {label_dropped})\")\n",
        "\n",
        "        df['language'] = lang_code  # Add language identifier\n",
        "\n",
        "        all_dataframes.append(df)\n",
        "        language_counts[lang_code] = len(df)\n",
        "\n",
        "        print(f\"✓ Loaded {lang_code}: {len(df)} samples from {file_name}\")\n",
        "\n",
        "    if not all_dataframes:\n",
        "        raise ValueError(\"No multilingual data loaded. Please verify the data directory and language list.\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TOTAL: {len(combined_df)} samples across {len(language_counts)} languages\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Show class distribution\n",
        "    if 'polarization' in combined_df.columns:\n",
        "        print(\"\\nClass Distribution:\")\n",
        "        for lang_code, count in language_counts.items():\n",
        "            lang_df = combined_df[combined_df['language'] == lang_code]\n",
        "            polarized = (lang_df['polarization'] == 1).sum()\n",
        "            non_polarized = (lang_df['polarization'] == 0).sum()\n",
        "            print(f\"  {lang_code}: Polarized={polarized}, Non-Polarized={non_polarized}\")\n",
        "\n",
        "    return combined_df, language_counts\n",
        "\n",
        "\n",
        "def generate_multilingual_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dev_dir,\n",
        "    output_dir,\n",
        "    languages=None,\n",
        "    threshold=0.48\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate predictions for all languages in dev folder\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        tokenizer: Tokenizer\n",
        "        dev_dir: Path to dev folder\n",
        "        output_dir: Where to save predictions\n",
        "        languages: List of language codes or None for all\n",
        "        threshold: Classification threshold\n",
        "\n",
        "    Returns:\n",
        "        all_predictions: Dict with predictions per language\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Language files\n",
        "    lang_files = {\n",
        "        'amh': 'amh.csv',\n",
        "        'arb': 'arb.csv',\n",
        "        'deu': 'deu.csv',\n",
        "        'eng': 'eng.csv',\n",
        "        'hau': 'hau.csv',\n",
        "        'ita': 'ita.csv',\n",
        "        'spa': 'spa.csv',\n",
        "        'urd': 'urd.csv',\n",
        "        'zho': 'zho.csv',\n",
        "    }\n",
        "\n",
        "    if languages is None:\n",
        "        languages = list(lang_files.keys())\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"GENERATING MULTILINGUAL PREDICTIONS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Languages: {', '.join(languages)}\")\n",
        "    print(f\"Dev directory: {dev_dir}\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(f\"Threshold: {threshold}\")\n",
        "    print()\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    all_predictions = {}\n",
        "\n",
        "    for lang_code in languages:\n",
        "        file_name = lang_files.get(lang_code)\n",
        "        if file_name is None:\n",
        "            continue\n",
        "\n",
        "        input_path = os.path.join(dev_dir, file_name)\n",
        "\n",
        "        if not os.path.exists(input_path):\n",
        "            print(f\"⚠️  Skipping {lang_code}: File not found\")\n",
        "            continue\n",
        "\n",
        "        # Output filename: pred_<lang>.csv\n",
        "        output_filename = f\"pred_{file_name}\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        print(f\"Processing {lang_code}...\")\n",
        "\n",
        "        # Load test data\n",
        "        test_df = pd.read_csv(input_path)\n",
        "\n",
        "        if 'text' not in test_df.columns or test_df['text'].dropna().empty:\n",
        "            print(f\"⚠️  Skipping {lang_code}: No valid text column in {input_path}\")\n",
        "            continue\n",
        "\n",
        "        # Clean text column\n",
        "        test_df['text'] = test_df['text'].astype(str).str.strip()\n",
        "        test_df = test_df[test_df['text'] != ''].copy()\n",
        "\n",
        "        if test_df.empty:\n",
        "            print(f\"⚠️  Skipping {lang_code}: All rows empty after cleaning\")\n",
        "            continue\n",
        "\n",
        "        # Create dataset\n",
        "        test_dataset = PolarizationDataset(\n",
        "            test_df['text'].tolist(),\n",
        "            [0] * len(test_df),  # Dummy labels for test\n",
        "            tokenizer,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        # Generate predictions\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        predictions = []\n",
        "        probabilities = []\n",
        "\n",
        "        from torch.utils.data import DataLoader\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            collate_fn=data_collator\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
        "                preds = (probs >= threshold).astype(int)\n",
        "\n",
        "                predictions.extend(preds)\n",
        "                probabilities.extend(probs)\n",
        "\n",
        "        # Create submission DataFrame\n",
        "        submission_df = pd.DataFrame({\n",
        "            'id': test_df['id'] if 'id' in test_df.columns else range(len(test_df)),\n",
        "            'text': test_df['text'],\n",
        "            'polarization': predictions,\n",
        "            'probability': probabilities\n",
        "        })\n",
        "\n",
        "        # Save predictions\n",
        "        submission_df.to_csv(output_path, index=False)\n",
        "\n",
        "        # Store results\n",
        "        all_predictions[lang_code] = submission_df\n",
        "\n",
        "        print(f\"✓ Saved {lang_code}: {len(submission_df)} predictions to {output_filename}\")\n",
        "        print(f\"  Polarized: {submission_df['polarization'].sum()}, Non-Polarized: {(submission_df['polarization']==0).sum()}\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ALL PREDICTIONS COMPLETED!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "    return all_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8df3d78",
      "metadata": {
        "id": "d8df3d78"
      },
      "source": [
        "## 2. Data Loading & Prediction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1e6317e5",
      "metadata": {
        "id": "1e6317e5"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: RWKV Architecture - Bidirectional WKV Kernel\n",
        "# ============================================================================\n",
        "def rwkv_linear_attention_bidirectional(r, k, v, w, u, attention_mask=None):\n",
        "    \"\"\"\n",
        "    Optimized bidirectional WKV with reduced memory footprint.\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, hidden_size = k.shape\n",
        "    device = k.device\n",
        "\n",
        "    # Expand w and u\n",
        "    w = w.unsqueeze(0).unsqueeze(0)  # (1, 1, hidden)\n",
        "    u = u.unsqueeze(0).unsqueeze(0)  # (1, 1, hidden)\n",
        "\n",
        "    # Handle attention mask\n",
        "    if attention_mask is None:\n",
        "        attention_mask = torch.ones(batch_size, seq_len, device=device)\n",
        "\n",
        "    mask = attention_mask.unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "\n",
        "    # Compute all pairwise distances at once\n",
        "    positions = torch.arange(seq_len, device=device).float()\n",
        "    distances = torch.abs(positions.unsqueeze(0) - positions.unsqueeze(1))  # (seq_len, seq_len)\n",
        "\n",
        "    # Compute decay weights for all positions\n",
        "    decay_weights = torch.exp(-(distances.unsqueeze(-1) + 1) * w)  # (seq_len, seq_len, hidden)\n",
        "\n",
        "    # Apply current token bonus on diagonal\n",
        "    current_bonus = torch.exp(u)  # (1, 1, hidden)\n",
        "    diagonal_mask = torch.eye(seq_len, device=device).unsqueeze(-1)  # (seq_len, seq_len, 1)\n",
        "    decay_weights = decay_weights * (1 - diagonal_mask) + current_bonus * diagonal_mask\n",
        "\n",
        "    # Broadcast mask: (batch, seq_len, 1) -> (batch, seq_len, seq_len, hidden)\n",
        "    mask_expanded = mask.unsqueeze(1) * mask.unsqueeze(2)  # (batch, seq_len, seq_len, 1)\n",
        "    decay_weights = decay_weights.unsqueeze(0) * mask_expanded  # (batch, seq_len, seq_len, hidden)\n",
        "\n",
        "    # Compute k*v product\n",
        "    kv = k.unsqueeze(1) * v.unsqueeze(1)  # (batch, 1, seq_len, hidden)\n",
        "\n",
        "    # Weighted sum: for each position, sum over all other positions\n",
        "    numerator = (decay_weights * kv).sum(dim=2)  # (batch, seq_len, hidden)\n",
        "\n",
        "    # Normalization denominator\n",
        "    denominator = (decay_weights * k.unsqueeze(1)).sum(dim=2)  # (batch, seq_len, hidden)\n",
        "    denominator = denominator.clamp(min=1e-8)\n",
        "\n",
        "    # Compute WKV\n",
        "    wkv = numerator / denominator\n",
        "\n",
        "    # Apply receptance gating\n",
        "    output = r * wkv\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abacb564",
      "metadata": {
        "id": "abacb564"
      },
      "source": [
        "## 3. RWKV Architecture Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56602b52",
      "metadata": {
        "id": "56602b52"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: RWKV Configuration\n",
        "# ============================================================================\n",
        "class RwkvBertConfig(PretrainedConfig):\n",
        "    \"\"\"Configuration for the RWKV polarization model.\"\"\"\n",
        "    model_type = \"rwkv_bert\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_dropout_prob=0.1,\n",
        "        layer_norm_eps=1e-12,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        initializer_range = kwargs.pop(\"initializer_range\", 0.02)\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "de32fab8",
      "metadata": {
        "id": "de32fab8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: RWKV Self-Attention & Feed-Forward\n",
        "# ============================================================================\n",
        "class RwkvBertSelfAttention(nn.Module):\n",
        "    \"\"\"Bidirectional RWKV attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        # Projection layers for R, K, V\n",
        "        self.receptance = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "        # Learnable time decay and bonus parameters\n",
        "        self.time_decay = nn.Parameter(torch.randn(config.hidden_size))\n",
        "        self.time_first = nn.Parameter(torch.randn(config.hidden_size))\n",
        "\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Handle attention mask shape\n",
        "        if attention_mask is not None and attention_mask.dim() == 4:\n",
        "            attention_mask = attention_mask.squeeze(1).squeeze(1)\n",
        "\n",
        "        # Project to R, K, V\n",
        "        r = self.receptance(hidden_states)\n",
        "        k = self.key(hidden_states)\n",
        "        v = self.value(hidden_states)\n",
        "\n",
        "        # Apply bidirectional WKV\n",
        "        rwkv_output = rwkv_linear_attention_bidirectional(\n",
        "            r, k, v,\n",
        "            self.time_decay,\n",
        "            self.time_first,\n",
        "            attention_mask\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        attention_output = self.output(rwkv_output)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class RwkvBertFeedForward(nn.Module):\n",
        "    \"\"\"RWKV channel mixing block.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # Feed-forward layers\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "\n",
        "        # Gating mechanism\n",
        "        self.gate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # Channel mixing with gating\n",
        "        x = self.fc1(hidden_states)\n",
        "        gate_value = self.gate(hidden_states)\n",
        "\n",
        "        # Apply activation and gating\n",
        "        x = self.activation(x) * torch.sigmoid(gate_value)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.fc2(x)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da680a6b",
      "metadata": {
        "id": "da680a6b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: RWKV Block & Encoder\n",
        "# ============================================================================\n",
        "class RwkvBertBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.ln2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.attention = RwkvBertSelfAttention(config)\n",
        "        self.ffn = RwkvBertFeedForward(config)\n",
        "        self.gradient_checkpointing = False  # Add this\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            # Use gradient checkpointing to save memory\n",
        "            def create_custom_forward(module):\n",
        "                def custom_forward(*inputs):\n",
        "                    return module(*inputs)\n",
        "                return custom_forward\n",
        "\n",
        "            attention_output = torch.utils.checkpoint.checkpoint(\n",
        "                create_custom_forward(self.attention),\n",
        "                self.ln1(hidden_states),\n",
        "                attention_mask\n",
        "            )\n",
        "        else:\n",
        "            attention_output = self.attention(self.ln1(hidden_states), attention_mask)\n",
        "\n",
        "        hidden_states = hidden_states + attention_output\n",
        "        ffn_output = self.ffn(self.ln2(hidden_states))\n",
        "        hidden_states = hidden_states + ffn_output\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RwkvBertEncoder(nn.Module):\n",
        "    \"\"\"Stack of RWKV blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([\n",
        "            RwkvBertBlock(config) for _ in range(config.num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "\n",
        "        for layer_module in self.layer:\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states] if v is not None)\n",
        "\n",
        "        from transformers.modeling_outputs import BaseModelOutput\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a6b907",
      "metadata": {
        "id": "f4a6b907"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: RWKV Model\n",
        "# ============================================================================\n",
        "class RwkvBertModel(PreTrainedModel):\n",
        "    \"\"\"Pure RWKV encoder with custom embeddings and pooler.\"\"\"\n",
        "    config_class = RwkvBertConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        # Custom embeddings\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Custom pooler\n",
        "        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.pooler_activation = nn.Tanh()\n",
        "\n",
        "        # RWKV encoder stack\n",
        "        self.encoder = RwkvBertEncoder(config)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.post_init()\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        if input_ids is None and inputs_embeds is None:\n",
        "            raise ValueError(\"You must provide input_ids or inputs_embeds.\")\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states\n",
        "            if output_hidden_states is not None\n",
        "            else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            device = input_ids.device\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "            device = inputs_embeds.device\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(batch_size, seq_length, device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(batch_size, seq_length, dtype=torch.long, device=device)\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "\n",
        "        position_embeds = self.position_embeddings(position_ids)\n",
        "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embedding_output = inputs_embeds + position_embeds + token_type_embeds\n",
        "        embedding_output = self.LayerNorm(embedding_output)\n",
        "        embedding_output = self.dropout(embedding_output)\n",
        "\n",
        "        if attention_mask.dim() == 2:\n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            extended_attention_mask = attention_mask\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = (\n",
        "            encoder_outputs[0] if not return_dict else encoder_outputs.last_hidden_state\n",
        "        )\n",
        "\n",
        "        first_token = sequence_output[:, 0]\n",
        "        pooled_output = self.pooler(first_token)\n",
        "        pooled_output = self.pooler_activation(pooled_output)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPooling(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7337a79f",
      "metadata": {
        "id": "7337a79f"
      },
      "outputs": [],
      "source": [
        "class BitLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    1.58-bit Quantized Linear Layer (BitNet)\n",
        "\n",
        "    Key Features:\n",
        "    - Weights: Ternary quantization {-1, 0, +1}\n",
        "    - Activations: 8-bit quantization [-128, 127]\n",
        "    - Straight-Through Estimator (STE) for gradient flow\n",
        "    - Lambda warmup for gradual quantization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Initialize weights with Xavier uniform (better for deep networks)\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Layer normalization before quantization (critical for stability)\n",
        "        self.layer_norm = nn.LayerNorm(in_features)\n",
        "\n",
        "        # Lambda for gradual quantization warmup (starts at 0, goes to 1)\n",
        "        self.register_buffer('lambda_val', torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Normalize input\n",
        "        x_norm = self.layer_norm(x)\n",
        "\n",
        "        # Quantize weights to {-1, 0, +1}\n",
        "        w_mean = self.weight.abs().mean()\n",
        "        w_scale = 1.0 / (w_mean + 1e-5)\n",
        "        w_quant = torch.clamp(torch.round(self.weight * w_scale), -1, 1) / w_scale\n",
        "\n",
        "        # Mix quantized and full-precision weights (warmup)\n",
        "        w_mixed = self.lambda_val * w_quant + (1 - self.lambda_val) * self.weight\n",
        "\n",
        "        # Quantize activations to 8-bit\n",
        "        x_max = x_norm.abs().max(dim=-1, keepdim=True)[0]\n",
        "        x_scale = 127.0 / (x_max + 1e-5)\n",
        "        x_quant = torch.clamp(torch.round(x_norm * x_scale), -128, 127) / x_scale\n",
        "\n",
        "        # Linear operation\n",
        "        output = F.linear(x_quant, w_mixed, self.bias)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f001d516",
      "metadata": {
        "id": "f001d516"
      },
      "source": [
        "## 4. BitNet & Classification Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3cbaaaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: BitNet Binary Classifier with RWKV\n",
        "# ============================================================================\n",
        "class BitNetBinaryClassifierRWKV(nn.Module):\n",
        "    \"\"\"Binary classifier using a pure RWKV encoder with BitLinear head.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-multilingual-cased\", num_labels=2, dropout_prob=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Initializing pure RWKV model (tokenizer base: {model_name})\")\n",
        "\n",
        "        rwkv_config = RwkvBertConfig(\n",
        "            vocab_size=30522,\n",
        "            hidden_size=768,\n",
        "            num_hidden_layers=12,\n",
        "            intermediate_size=3072,\n",
        "            hidden_dropout_prob=dropout_prob,\n",
        "            layer_norm_eps=1e-12,\n",
        "            max_position_embeddings=512,\n",
        "            type_vocab_size=2,\n",
        "        )\n",
        "\n",
        "        # Initialize RWKV encoder with random weights\n",
        "        self.bert = RwkvBertModel(rwkv_config)\n",
        "        print(\"✓ Initialized RWKV model with random weights (no pretrained components)\")\n",
        "\n",
        "        config = self.bert.config\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # BitLinear classification head\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.bitfc1 = BitLinear(config.hidden_size, config.hidden_size // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.bitfc2 = BitLinear(config.hidden_size // 2, num_labels)\n",
        "\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"RWKV Model initialized with {total_params:,} parameters\")\n",
        "        print(\"  - Encoder: RWKV (Bi-WKV attention, O(N) complexity)\")\n",
        "        print(\"  - Classifier: BitLinear (1.58-bit quantization)\")\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        \"\"\"Compute logits (and optional loss) for the RWKV classifier.\"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.bert.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        hidden = self.bitfc1(pooled_output)\n",
        "        hidden = self.activation(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        logits = self.bitfc2(hidden)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d4a0f563",
      "metadata": {
        "id": "d4a0f563"
      },
      "outputs": [],
      "source": [
        "class PolarizationDataset(Dataset):\n",
        "    \"\"\"Dataset class for polarization detection.\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx]).strip()\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        if not text:\n",
        "            raise ValueError(f\"Sample {idx} has empty text after stripping; please clean the dataset.\")\n",
        "\n",
        "        # Tokenize - return_tensors should be None (default) for list output\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=None  # This ensures lists, not tensors\n",
        "        )\n",
        "\n",
        "        # Directly add label to the encoding dictionary\n",
        "        encoding['labels'] = label\n",
        "\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "77a5eaab",
      "metadata": {
        "id": "77a5eaab"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for handling class imbalance\n",
        "    Reference: https://arxiv.org/abs/1708.02002\n",
        "\n",
        "    Better than weighted CE for imbalanced classification because it:\n",
        "    - Focuses on hard-to-classify examples\n",
        "    - Down-weights easy examples\n",
        "    - Reduces false positives\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.65, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d8f60864",
      "metadata": {
        "id": "d8f60864"
      },
      "outputs": [],
      "source": [
        "class BitNetTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom trainer with:\n",
        "    - Gradual quantization warmup (lambda scheduling)\n",
        "    - Option for Weighted CE or Focal Loss for class imbalance\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup_steps=1000, class_weight=None, use_focal_loss=False, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.use_focal_loss = use_focal_loss\n",
        "\n",
        "        # Handle class weights\n",
        "        if class_weight is not None:\n",
        "            self.class_weight = class_weight.to(self.args.device)\n",
        "        else:\n",
        "            self.class_weight = None\n",
        "\n",
        "        print(f\"Lambda warmup enabled: 0 -> 1 over {warmup_steps} steps\")\n",
        "\n",
        "        # Initialize loss function\n",
        "        if self.use_focal_loss:\n",
        "            self.focal_loss = FocalLoss(alpha=0.65, gamma=2.0)\n",
        "            print(f\"Using Focal Loss (alpha=0.65, gamma=2.0)\")\n",
        "        elif self.class_weight is not None:\n",
        "            print(f\"Using Weighted CE Loss with weights: {self.class_weight}\")\n",
        "        else:\n",
        "            print(f\"Using standard Cross-Entropy Loss\")\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Calculate lambda based on current training step\n",
        "        current_step = self.state.global_step\n",
        "        lambda_val = min(1.0, current_step / self.warmup_steps)\n",
        "\n",
        "        # Set lambda for all BitLinear layers\n",
        "        for module in model.modules():\n",
        "            if hasattr(module, 'lambda_val'):\n",
        "                module.lambda_val.fill_(lambda_val)\n",
        "\n",
        "        # Get labels and perform forward pass\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss based on selected method\n",
        "        if self.use_focal_loss:\n",
        "            loss = self.focal_loss(logits, labels)\n",
        "        elif self.class_weight is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weight)\n",
        "            loss = loss_fct(logits, labels)\n",
        "        else:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for binary classification\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return {\n",
        "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "        'f1_binary': f1_score(labels, preds, average='binary'),\n",
        "        'accuracy': accuracy_score(labels, preds)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9ee112",
      "metadata": {
        "id": "eb9ee112"
      },
      "outputs": [],
      "source": [
        "def train_multilingual_polarization_detector(\n",
        "    train_dir='/content/gdrive/MyDrive/subtask1/train',\n",
        "    languages=None,  # None = all languages\n",
        "    model_name='bert-base-multilingual-cased',\n",
        "    use_lr_finder=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Train multilingual polarization detector with a pure RWKV encoder backbone.\n",
        "\n",
        "    Args:\n",
        "        train_dir: Path to training data folder\n",
        "        languages: List of language codes or None for all\n",
        "        model_name: Tokenizer checkpoint to align vocabularies\n",
        "        use_lr_finder: Whether to run LR finder\n",
        "    \"\"\"\n",
        "    set_seed(42)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RWKV MULTILINGUAL POLARIZATION DETECTION TRAINING\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # STEP 1: Load multilingual data\n",
        "    print(\"STEP 1: LOADING MULTILINGUAL DATA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    train_full, lang_counts = load_multilingual_data(\n",
        "        data_dir=train_dir,\n",
        "        languages=languages,\n",
        "        split='train'\n",
        "    )\n",
        "\n",
        "    # Stratified split preserving language distribution\n",
        "    train, val = train_test_split(\n",
        "        train_full,\n",
        "        test_size=0.2,\n",
        "        stratify=train_full[['polarization', 'language']],  # Stratify by both\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain samples: {len(train)}\")\n",
        "    print(f\"Val samples: {len(val)}\")\n",
        "\n",
        "    # STEP 2: Initialize tokenizer and model\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"STEP 2: INITIALIZING RWKV MODEL\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Tokenizer checkpoint: {model_name}\")\n",
        "    print(\"Architecture: RWKV encoder with BitLinear head (O(N) complexity)\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    train_dataset = PolarizationDataset(\n",
        "        train['text'].tolist(),\n",
        "        train['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    val_dataset = PolarizationDataset(\n",
        "        val['text'].tolist(),\n",
        "        val['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Initialize RWKV model\n",
        "    model = BitNetBinaryClassifierRWKV(\n",
        "        model_name=model_name,\n",
        "        num_labels=2,\n",
        "        dropout_prob=0.2\n",
        "    )\n",
        "\n",
        "    # STEP 3: Learning Rate Finder (optional)\n",
        "    if use_lr_finder:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"STEP 3: LEARNING RATE FINDER\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        suggested_lr, lr_results = find_optimal_learning_rate(\n",
        "            model=model,\n",
        "            train_dataset=train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            start_lr=1e-10,\n",
        "            end_lr=1e-1,\n",
        "            num_iter=2000,\n",
        "            plot=True\n",
        "        )\n",
        "\n",
        "        final_lr = suggested_lr\n",
        "        print(f\"\\nUsing Learning Rate: {final_lr:.2e}\")\n",
        "    else:\n",
        "        final_lr = 3e-5\n",
        "        print(f\"\\nUsing default Learning Rate: {final_lr:.2e}\")\n",
        "\n",
        "    # STEP 4: Train model\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"STEP 4: TRAINING RWKV MODEL\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    class_weights = torch.tensor([0.82, 1.30], dtype=torch.float32)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results_rwkv_multilingual',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=200,\n",
        "        learning_rate=final_lr,\n",
        "        weight_decay=0.02,\n",
        "        logging_dir='./logs_rwkv_multilingual',\n",
        "        logging_steps=50,\n",
        "        eval_strategy='steps',\n",
        "        eval_steps=150,\n",
        "        save_strategy='steps',\n",
        "        save_steps=150,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1_macro',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        report_to='none',\n",
        "        seed=42,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    trainer = BitNetTrainer(\n",
        "        warmup_steps=1000,\n",
        "        class_weight=class_weights,\n",
        "        use_focal_loss=True,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    )\n",
        "\n",
        "    results = trainer.train()\n",
        "\n",
        "    return model, tokenizer, trainer, results, train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ba1f27",
      "metadata": {
        "id": "c3ba1f27"
      },
      "outputs": [],
      "source": [
        "def predict_polarization(text, model, tokenizer, return_probabilities=True):\n",
        "    \"\"\"\n",
        "    Make predictions on new text using the RWKV BitNet model.\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "        model: Trained RWKV BitNet model\n",
        "        tokenizer: Hugging Face tokenizer aligned with the model vocabulary\n",
        "        return_probabilities: If True, return probabilities along with prediction\n",
        "\n",
        "    Returns:\n",
        "        prediction: 0 (Not Polarized) or 1 (Polarized)\n",
        "        confidence: Probability of being polarized (if return_probabilities=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        pred = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][1].item()  # Probability of being polarized\n",
        "\n",
        "    if return_probabilities:\n",
        "        return pred, confidence\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e670dc06",
      "metadata": {
        "id": "e670dc06"
      },
      "outputs": [],
      "source": [
        "def save_model_to_drive(model, tokenizer, save_dir, model_config, threshold=None):\n",
        "    \"\"\"\n",
        "    Save complete RWKV model to Google Drive for later inference\n",
        "\n",
        "    Args:\n",
        "        model: Trained BitNetBinaryClassifierRWKV\n",
        "        tokenizer: AutoTokenizer\n",
        "        save_dir: Path in Google Drive\n",
        "        model_config: Dict with model configuration\n",
        "        threshold: Optimal threshold (optional)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving model to {save_dir}...\")\n",
        "\n",
        "    # 1. Save PyTorch model state dict\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        os.path.join(save_dir, 'pytorch_model.bin')\n",
        "    )\n",
        "    print(\"✓ Saved PyTorch model weights\")\n",
        "\n",
        "    # 2. Save tokenizer (HuggingFace format)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(\"✓ Saved tokenizer\")\n",
        "\n",
        "    # 3. Save model configuration\n",
        "    config_with_threshold = {\n",
        "        **model_config,\n",
        "        'optimal_threshold': threshold,\n",
        "        'saved_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, 'model_config.json'), 'w') as f:\n",
        "        json.dump(config_with_threshold, f, indent=2)\n",
        "    print(\"✓ Saved model configuration\")\n",
        "\n",
        "    # 4. Save training metrics (if available)\n",
        "    metrics_file = os.path.join(save_dir, 'training_metrics.txt')\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        f.write(f\"Model Configuration:\\n\")\n",
        "        f.write(f\"Model: {model_config['model_name']}\\n\")\n",
        "        f.write(f\"Dropout: {model_config['dropout_prob']}\\n\")\n",
        "        f.write(f\"Optimal Threshold: {threshold}\\n\")\n",
        "        f.write(f\"Saved: {config_with_threshold['saved_date']}\\n\")\n",
        "    print(\"✓ Saved training metrics\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MODEL SUCCESSFULLY SAVED TO GOOGLE DRIVE!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Location: {save_dir}\")\n",
        "    print(f\"Files saved:\")\n",
        "    print(f\"  - pytorch_model.bin (model weights)\")\n",
        "    print(f\"  - tokenizer files (tokenizer_config.json, vocab.txt, etc.)\")\n",
        "    print(f\"  - model_config.json (configuration)\")\n",
        "    print(f\"  - training_metrics.txt (metadata)\")\n",
        "\n",
        "\n",
        "def load_model_from_drive(save_dir):\n",
        "    \"\"\"\n",
        "    Load trained RWKV model from Google Drive for inference\n",
        "\n",
        "    Args:\n",
        "        save_dir: Path where model was saved in Google Drive\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded BitNetBinaryClassifierRWKV\n",
        "        tokenizer: Loaded tokenizer\n",
        "        config: Model configuration dict\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    print(f\"Loading model from {save_dir}...\")\n",
        "\n",
        "    # 1. Load model configuration\n",
        "    config_path = os.path.join(save_dir, 'model_config.json')\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    print(f\"✓ Loaded configuration\")\n",
        "\n",
        "    # 2. Initialize RWKV model with same architecture\n",
        "    model = BitNetBinaryClassifierRWKV(\n",
        "        model_name=config['model_name'],\n",
        "        num_labels=config['num_labels'],\n",
        "        dropout_prob=config['dropout_prob']\n",
        "    )\n",
        "    print(f\"✓ Initialized model architecture\")\n",
        "\n",
        "    # 3. Load model weights\n",
        "    model_path = os.path.join(save_dir, 'pytorch_model.bin')\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    print(f\"✓ Loaded model weights\")\n",
        "\n",
        "    # 4. Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "    print(f\"✓ Loaded tokenizer\")\n",
        "\n",
        "    # Move model to appropriate device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    print(f\"✓ Model moved to {device}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MODEL SUCCESSFULLY LOADED!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Model: {config['model_name']}\")\n",
        "    print(f\"Optimal Threshold: {config.get('optimal_threshold', 'Not saved')}\")\n",
        "    print(f\"Saved Date: {config.get('saved_date', 'Unknown')}\")\n",
        "\n",
        "    return model, tokenizer, config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4253c499",
      "metadata": {
        "id": "4253c499"
      },
      "outputs": [],
      "source": [
        "def find_optimal_learning_rate(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    tokenizer,\n",
        "    start_lr=1e-10,\n",
        "    end_lr=1e-1,\n",
        "    num_iter=2000,\n",
        "    plot=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Learning Rate Finder compatible with HuggingFace transformers\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"LEARNING RATE RANGE TEST\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Testing learning rates from {start_lr} to {end_lr}\")\n",
        "    print(f\"Number of iterations: {num_iter}\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=start_lr,\n",
        "        weight_decay=0.02\n",
        "    )\n",
        "\n",
        "    # Loss function\n",
        "    criterion = FocalLoss(alpha=0.65, gamma=2.0)\n",
        "\n",
        "    # Create data loader with HuggingFace collator\n",
        "    from torch.utils.data import DataLoader\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    # Generate LR values (exponential spacing)\n",
        "    lrs = np.logspace(np.log10(start_lr), np.log10(end_lr), num_iter)\n",
        "\n",
        "    # Storage\n",
        "    losses = []\n",
        "    learning_rates = []\n",
        "\n",
        "    print(f\"\\nRunning LR Range Test...\")\n",
        "\n",
        "    data_iter = iter(train_loader)\n",
        "\n",
        "    for i, lr in enumerate(lrs):\n",
        "        # Update learning rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        try:\n",
        "            # Get batch - this is BatchEncoding format from HuggingFace\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(train_loader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        # Extract data from BatchEncoding properly\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store results\n",
        "        losses.append(loss.item())\n",
        "        learning_rates.append(lr)\n",
        "\n",
        "        # Print progress\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"  Step {i+1}/{num_iter}: LR = {lr:.2e}, Loss = {loss.item():.4f}\")\n",
        "\n",
        "        # Stop if loss explodes\n",
        "        if loss.item() > 100:\n",
        "            print(f\"\\nStopped early at step {i+1}: Loss exploded\")\n",
        "            break\n",
        "\n",
        "    # Find best LR (steepest negative gradient)\n",
        "    loss_gradients = np.gradient(losses)\n",
        "    best_idx = np.argmin(loss_gradients)\n",
        "    suggested_lr = learning_rates[best_idx]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SUGGESTED LEARNING RATE: {suggested_lr:.2e}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Plot if requested\n",
        "    if plot:\n",
        "        try:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "            # Plot 1: Loss vs LR\n",
        "            axes[0].plot(learning_rates, losses, 'b-')\n",
        "            axes[0].axvline(x=suggested_lr, color='red', linestyle='--',\n",
        "                           label=f'Suggested: {suggested_lr:.2e}')\n",
        "            axes[0].set_xlabel('Learning Rate')\n",
        "            axes[0].set_ylabel('Loss')\n",
        "            axes[0].set_title('Loss vs Learning Rate')\n",
        "            axes[0].set_xscale('log')\n",
        "            axes[0].legend()\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 2: Loss gradient\n",
        "            axes[1].plot(learning_rates, loss_gradients, 'g-')\n",
        "            axes[1].axvline(x=suggested_lr, color='red', linestyle='--',\n",
        "                           label=f'Suggested: {suggested_lr:.2e}')\n",
        "            axes[1].set_xlabel('Learning Rate')\n",
        "            axes[1].set_ylabel('Loss Gradient')\n",
        "            axes[1].set_title('Loss Gradient')\n",
        "            axes[1].set_xscale('log')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('lr_finder_results.png', dpi=150)\n",
        "            print(f\"\\nPlot saved to: lr_finder_results.png\")\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create plot: {e}\")\n",
        "\n",
        "    # Store results\n",
        "    lr_results = {\n",
        "        'learning_rates': learning_rates,\n",
        "        'losses': losses,\n",
        "        'suggested_lr': suggested_lr\n",
        "    }\n",
        "\n",
        "    # Save to CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'learning_rate': learning_rates,\n",
        "        'loss': losses\n",
        "    })\n",
        "    results_df.to_csv('lr_finder_results.csv', index=False)\n",
        "    print(f\"Results saved to: lr_finder_results.csv\")\n",
        "\n",
        "    return suggested_lr, lr_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782e9bf4",
      "metadata": {
        "id": "782e9bf4"
      },
      "source": [
        "## 5. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13dfa3da",
      "metadata": {
        "id": "13dfa3da"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Configuration\n",
        "    TRAIN_DIR = '/content/gdrive/MyDrive/subtask1/train'\n",
        "    DEV_DIR = '/content/gdrive/MyDrive/subtask1/dev'\n",
        "    OUTPUT_DIR = '/content/gdrive/MyDrive/subtask1/predictions'\n",
        "    MODEL_SAVE_DIR = '/content/gdrive/MyDrive/SemevalModels/bitnet_multilingual'\n",
        "    TOKENIZER_CHECKPOINT = 'bert-base-multilingual-cased'\n",
        "\n",
        "    INFERENCE_MODE = False  # Set to True to skip training\n",
        "    USE_LR_FINDER = False  # Set to True to find optimal LR\n",
        "\n",
        "    # Language selection (None = all languages)\n",
        "    LANGUAGES = None  # Or specify: ['eng', 'arb', 'deu', 'spa']\n",
        "\n",
        "    if INFERENCE_MODE:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"INFERENCE-ONLY MODE - RWKV MULTILINGUAL\")\n",
        "        print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "        model, tokenizer, config = load_model_from_drive(MODEL_SAVE_DIR)\n",
        "        threshold = config.get('optimal_threshold', 0.48)\n",
        "\n",
        "        generate_multilingual_predictions(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            dev_dir=DEV_DIR,\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            languages=LANGUAGES,\n",
        "            threshold=threshold\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"FULL RWKV MULTILINGUAL TRAINING WORKFLOW\")\n",
        "        print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "        model, tokenizer, trainer, train_results, train_df, val_df = train_multilingual_polarization_detector(\n",
        "            train_dir=TRAIN_DIR,\n",
        "            languages=LANGUAGES,\n",
        "            model_name=TOKENIZER_CHECKPOINT,\n",
        "            use_lr_finder=USE_LR_FINDER\n",
        "        )\n",
        "\n",
        "        val_df.to_csv('val_temp.csv', index=False)\n",
        "        optimal_threshold, best_f1_macro, threshold_results = find_optimal_threshold(\n",
        "            model, tokenizer, val_file='val_temp.csv'\n",
        "        )\n",
        "\n",
        "        print(f\"\\nOptimal threshold: {optimal_threshold:.2f}\")\n",
        "        print(f\"Validation F1 Macro: {best_f1_macro:.4f}\")\n",
        "\n",
        "        model_config = {\n",
        "            'tokenizer_checkpoint': TOKENIZER_CHECKPOINT,\n",
        "            'num_labels': 2,\n",
        "            'dropout_prob': 0.2,\n",
        "            'languages_trained': LANGUAGES if LANGUAGES else 'all'\n",
        "        }\n",
        "\n",
        "        save_model_to_drive(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            save_dir=MODEL_SAVE_DIR,\n",
        "            model_config=model_config,\n",
        "            threshold=optimal_threshold\n",
        "        )\n",
        "\n",
        "        generate_multilingual_predictions(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            dev_dir=DEV_DIR,\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            languages=LANGUAGES,\n",
        "            threshold=optimal_threshold\n",
        "        )\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"RWKV MULTILINGUAL TRAINING COMPLETE!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Model saved to: {MODEL_SAVE_DIR}\")\n",
        "        print(f\"Predictions saved to: {OUTPUT_DIR}\")\n",
        "        print(f\"Optimal threshold: {optimal_threshold:.2f}\")\n",
        "        print(f\"Validation F1 Macro: {best_f1_macro:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce4d37d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fce4d37d",
        "outputId": "cd7869e5-51de-498b-981e-0c1e565206b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "FULL RWKV MULTILINGUAL TRAINING WORKFLOW\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "RWKV-BERT MULTILINGUAL POLARIZATION DETECTION TRAINING\n",
            "======================================================================\n",
            "\n",
            "STEP 1: LOADING MULTILINGUAL DATA\n",
            "======================================================================\n",
            "======================================================================\n",
            "LOADING TRAIN DATA - MULTILINGUAL\n",
            "======================================================================\n",
            "Languages requested: amh, arb, deu, eng, hau, ita, spa, urd, zho\n",
            "Data directory: /content/gdrive/MyDrive/subtask1/train\n",
            "\n",
            "✓ Loaded amh: 3332 samples from amh.csv\n",
            "✓ Loaded arb: 3380 samples from arb.csv\n",
            "✓ Loaded deu: 3180 samples from deu.csv\n",
            "✓ Loaded eng: 2676 samples from eng.csv\n",
            "✓ Loaded hau: 3651 samples from hau.csv\n",
            "✓ Loaded ita: 3334 samples from ita.csv\n",
            "✓ Loaded spa: 3305 samples from spa.csv\n",
            "✓ Loaded urd: 2849 samples from urd.csv\n",
            "✓ Loaded zho: 4280 samples from zho.csv\n",
            "\n",
            "======================================================================\n",
            "TOTAL: 29987 samples across 9 languages\n",
            "======================================================================\n",
            "\n",
            "Class Distribution:\n",
            "  amh: Polarized=2518, Non-Polarized=814\n",
            "  arb: Polarized=1512, Non-Polarized=1868\n",
            "  deu: Polarized=1512, Non-Polarized=1668\n",
            "  eng: Polarized=1002, Non-Polarized=1674\n",
            "  hau: Polarized=392, Non-Polarized=3259\n",
            "  ita: Polarized=1368, Non-Polarized=1966\n",
            "  spa: Polarized=1660, Non-Polarized=1645\n",
            "  urd: Polarized=1976, Non-Polarized=873\n",
            "  zho: Polarized=2121, Non-Polarized=2159\n",
            "\n",
            "Train samples: 23989\n",
            "Val samples: 5998\n",
            "\n",
            "======================================================================\n",
            "STEP 2: INITIALIZING RWKV-BERT MODEL\n",
            "======================================================================\n",
            "Base Model: bert-base-multilingual-cased\n",
            "Architecture: RWKV-BERT Hybrid (O(N) complexity)\n",
            "Initializing RWKV-BERT Hybrid Model (base: bert-base-multilingual-cased)\n",
            "✓ Loaded pretrained BERT embeddings\n",
            "✓ Loaded pretrained BERT pooler\n",
            "✗ RWKV encoder initialized randomly (no pretrained weights)\n",
            "RWKV-BERT Model initialized with 206,518,658 parameters\n",
            "  - Encoder: RWKV (Bi-WKV attention, O(N) complexity)\n",
            "  - Classifier: BitLinear (1.58-bit quantization)\n",
            "\n",
            "Using default Learning Rate: 3.00e-05\n",
            "\n",
            "======================================================================\n",
            "STEP 4: TRAINING RWKV-BERT MODEL\n",
            "======================================================================\n",
            "Lambda warmup enabled: 0 -> 1 over 1000 steps\n",
            "Using Focal Loss (alpha=0.65, gamma=2.0)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='151' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 151/2250 02:08 < 30:13, 1.16 it/s, Epoch 0.20/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Binary</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.346979</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.531344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Placeholder cell: model configuration handled inside the main routine above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "637939f3",
      "metadata": {
        "id": "637939f3"
      },
      "source": [
        "## 6. Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fc4502",
      "metadata": {
        "id": "68fc4502"
      },
      "outputs": [],
      "source": [
        "def test_inference_examples(model, tokenizer):\n",
        "    \"\"\"Test RWKV model on example texts\"\"\"\n",
        "\n",
        "    test_examples = [\n",
        "        \"This politician is destroying our country with terrible policies!\",\n",
        "        \"I believe we need better education and healthcare systems.\",\n",
        "        \"Those people are all criminals and should be deported immediately!\",\n",
        "        \"Research shows that renewable energy can reduce carbon emissions.\",\n",
        "        \"They're trying to take away our rights and freedoms!\",\n",
        "        \"The weather forecast predicts rain tomorrow afternoon.\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RWKV INFERENCE EXAMPLES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, text in enumerate(test_examples, 1):\n",
        "        pred, confidence = predict_polarization(text, model, tokenizer)\n",
        "        label = \"Polarized\" if pred == 1 else \"Not Polarized\"\n",
        "        print(f\"\\n{i}. Text: {text}\")\n",
        "        print(f\"   Prediction: {label}\")\n",
        "        print(f\"   Confidence: {confidence:.3f}\")\n",
        "\n",
        "# Optional: Test model after training\n",
        "# test_inference_examples(model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
