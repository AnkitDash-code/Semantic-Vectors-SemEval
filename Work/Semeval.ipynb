{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "metadata": {
        "id": "Ls1UPn_tlIK8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OXapiv1YhQwN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.40.0 torch>=2.0.0 accelerate scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii-AjKfOhtRg",
        "outputId": "031932b7-4d75-4557-a845-fc32cf3e3b60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BitLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    1.58-bit Quantized Linear Layer (BitNet)\n",
        "\n",
        "    Key Features:\n",
        "    - Weights: Ternary quantization {-1, 0, +1}\n",
        "    - Activations: 8-bit quantization [-128, 127]\n",
        "    - Straight-Through Estimator (STE) for gradient flow\n",
        "    - Lambda warmup for gradual quantization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Initialize weights with Xavier uniform (better for deep networks)\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Layer normalization before quantization (critical for stability)\n",
        "        self.layer_norm = nn.LayerNorm(in_features)\n",
        "\n",
        "        # Lambda for gradual quantization warmup (starts at 0, goes to 1)\n",
        "        self.register_buffer('lambda_val', torch.tensor(0.0))\n",
        "\n",
        "    def weight_quant(self, w):\n",
        "        \"\"\"\n",
        "        Quantize weights to ternary values {-1, 0, +1}\n",
        "        Uses round-to-nearest with scale normalization\n",
        "        \"\"\"\n",
        "        # Calculate scale factor using mean absolute value\n",
        "        scale = 1.0 / w.abs().mean().clamp_(min=1e-5)\n",
        "        # Round to nearest integer and clamp to [-1, 0, 1]\n",
        "        w_quant = (w * scale).round().clamp_(-1, 1) / scale\n",
        "        return w_quant\n",
        "\n",
        "    def activation_quant(self, x):\n",
        "        \"\"\"\n",
        "        Quantize activations to 8-bit using absmax quantization\n",
        "        Maps to [-128, 127] range\n",
        "        \"\"\"\n",
        "        # Find maximum absolute value per sample\n",
        "        scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n",
        "        # Quantize and dequantize\n",
        "        x_quant = (x * scale).round().clamp_(-128, 127) / scale\n",
        "        return x_quant\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply layer normalization first\n",
        "        x_norm = self.layer_norm(x)\n",
        "\n",
        "        # Get current lambda value (controls quantization strength)\n",
        "        lambda_val = self.lambda_val.item()\n",
        "\n",
        "        if self.training:\n",
        "            # During training: gradual quantization with lambda warmup\n",
        "            x_quant_full = self.activation_quant(x_norm)\n",
        "            w_quant_full = self.weight_quant(self.weight)\n",
        "\n",
        "            # Linear interpolation between full precision and quantized\n",
        "            # Lambda = 0: full precision, Lambda = 1: full quantization\n",
        "            x_mixed = x_norm * (1 - lambda_val) + x_quant_full * lambda_val\n",
        "            w_mixed = self.weight * (1 - lambda_val) + w_quant_full * lambda_val\n",
        "\n",
        "            # Straight-Through Estimator: forward with quantized, backward with original\n",
        "            x_final = x_mixed + (x_quant_full - x_mixed).detach()\n",
        "            w_final = w_mixed + (w_quant_full - w_mixed).detach()\n",
        "        else:\n",
        "            # During inference: full quantization (lambda = 1)\n",
        "            x_final = x_norm + (self.activation_quant(x_norm) - x_norm).detach()\n",
        "            w_final = self.weight + (self.weight_quant(self.weight) - self.weight).detach()\n",
        "\n",
        "        # Standard linear transformation\n",
        "        return F.linear(x_final, w_final, self.bias)"
      ],
      "metadata": {
        "id": "Y7DDaUCKiQHQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BitNetBinaryClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    BitNet model for binary polarization detection\n",
        "    Architecture: BERT -> BitLinear Layers -> Classification\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained BERT\n",
        "        print(f\"Loading BERT model: {model_name}\")\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        config = self.bert.config\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Optional: Freeze early BERT layers for efficiency\n",
        "        # Uncomment to freeze first 8 layers (keeps last 4 trainable)\n",
        "        # for layer in self.bert.encoder.layer[:8]:\n",
        "        #     for param in layer.parameters():\n",
        "        #         param.requires_grad = False\n",
        "\n",
        "        # BitLinear classification head (2 layers for better representation)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.bit_fc1 = BitLinear(config.hidden_size, config.hidden_size // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.bit_fc2 = BitLinear(config.hidden_size // 2, num_labels)\n",
        "\n",
        "        print(f\"Model initialized with {sum(p.numel() for p in self.parameters()):,} parameters\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # Get BERT embeddings\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # Use [CLS] token representation (pooled output)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # Pass through BitLinear layers\n",
        "        x = self.bit_fc1(pooled_output)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.bit_fc2(x)\n",
        "\n",
        "        # Compute loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        # Return in HuggingFace format\n",
        "        return type('ModelOutput', (), {\n",
        "            'loss': loss,\n",
        "            'logits': logits,\n",
        "            'hidden_states': None,\n",
        "            'attentions': None\n",
        "        })()"
      ],
      "metadata": {
        "id": "REJvh3v8iZqv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolarizationDataset(Dataset):\n",
        "    \"\"\"Dataset class for polarization detection\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize with proper truncation\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=False,  # Handled by DataCollator\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Squeeze to remove batch dimension\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "p53peM3Hiier"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BitNetTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom trainer with gradual quantization warmup (lambda scheduling)\n",
        "    Lambda increases from 0 to 1 over warmup_steps\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup_steps=1000, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        print(f\"Lambda warmup enabled: 0 -> 1 over {warmup_steps} steps\")\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Calculate lambda based on current training step\n",
        "        current_step = self.state.global_step\n",
        "        lambda_val = min(current_step / self.warmup_steps, 1.0)\n",
        "\n",
        "        # Update all BitLinear layers with current lambda value\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, BitLinear):\n",
        "                module.lambda_val.data = torch.tensor(lambda_val, device=module.lambda_val.device)\n",
        "\n",
        "        # Log lambda value periodically\n",
        "        if current_step % 100 == 0:\n",
        "            self.log({'lambda_val': lambda_val})\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        \"\"\"\n",
        "        Override prediction_step to handle ModelOutput correctly.\n",
        "        \"\"\"\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            if isinstance(outputs, dict):\n",
        "                # Handle case where model output is a dictionary (e.g., for newer transformers versions)\n",
        "                loss = outputs.get(\"loss\")\n",
        "                logits = outputs.get(\"logits\")\n",
        "            else:\n",
        "                 # Handle custom ModelOutput object\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "\n",
        "            if prediction_loss_only:\n",
        "                return (loss, None, None) # Only return loss\n",
        "\n",
        "            labels = inputs.get(\"labels\")\n",
        "\n",
        "            if labels is not None:\n",
        "                 # Ensure labels are in the correct format if needed (though compute_metrics handles this)\n",
        "                 pass\n",
        "\n",
        "\n",
        "            return (loss, logits, labels) # Return loss, logits, and labels"
      ],
      "metadata": {
        "id": "3yCEKOGFinO9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for binary classification\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return {\n",
        "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
        "        'f1_binary': f1_score(labels, preds, average='binary'),\n",
        "        'accuracy': accuracy_score(labels, preds)\n",
        "    }"
      ],
      "metadata": {
        "id": "VKnCV3l3ir-M"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_polarization_detector():\n",
        "    \"\"\"\n",
        "    Main training function for Subtask 1: Binary Polarization Detection\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"SUBTASK 1: BINARY POLARIZATION DETECTION\")\n",
        "    print(\"1-Bit LLM with BitLinear Quantization\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\nLoading data...\")\n",
        "    train = pd.read_csv('eng.csv')\n",
        "    val = pd.read_csv('eng.csv')\n",
        "\n",
        "    print(f\"Training samples: {len(train)}\")\n",
        "    print(f\"Validation samples: {len(val)}\")\n",
        "    print(f\"\\nLabel distribution in training:\")\n",
        "    print(train['polarization'].value_counts())\n",
        "    print(f\"\\nClass balance: {train['polarization'].value_counts(normalize=True).to_dict()}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    print(\"\\nInitializing tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = PolarizationDataset(\n",
        "        train['text'].tolist(),\n",
        "        train['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    val_dataset = PolarizationDataset(\n",
        "        val['text'].tolist(),\n",
        "        val['polarization'].tolist(),\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Initialize BitNet model\n",
        "    print(\"\\nInitializing BitNet model...\")\n",
        "    model = BitNetBinaryClassifier(\n",
        "        model_name='bert-base-uncased',\n",
        "        num_labels=2,\n",
        "        dropout_prob=0.1\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Count BitLinear parameters\n",
        "    bitlinear_params = sum(p.numel() for n, p in model.named_parameters() if 'bit_fc' in n)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"BitLinear parameters: {bitlinear_params:,} ({100*bitlinear_params/total_params:.1f}% of total)\")\n",
        "\n",
        "    # Training arguments optimized for 1-bit quantization\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./bitnet_polarization\",\n",
        "\n",
        "        # Training hyperparameters\n",
        "        num_train_epochs=5,  # More epochs for quantization convergence\n",
        "        learning_rate=1e-4,  # Higher LR works better with quantization\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        gradient_accumulation_steps=2,  # Effective batch size = 64\n",
        "\n",
        "        # Evaluation and saving\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "\n",
        "        # Logging\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        report_to=None,  # Disable wandb\n",
        "\n",
        "        # Optimization\n",
        "        warmup_steps=500,  # Warmup for optimizer\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0,\n",
        "\n",
        "        # Precision (bf16 recommended for BitNet)\n",
        "        fp16=False,\n",
        "        bf16=True,  # Better numerical stability than fp16\n",
        "\n",
        "        # Performance\n",
        "        dataloader_num_workers=4,\n",
        "        dataloader_pin_memory=True,\n",
        "\n",
        "        # Misc\n",
        "        remove_unused_columns=False,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    # Initialize custom trainer with lambda warmup\n",
        "    print(\"\\nInitializing BitNet trainer with lambda warmup...\")\n",
        "    trainer = BitNetTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        warmup_steps=500  # Lambda warmup steps\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Starting training...\")\n",
        "    print(\"=\"*60)\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Print training summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training completed!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
        "    print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VALIDATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"F1 Macro:  {eval_results['eval_f1_macro']:.4f}\")\n",
        "    print(f\"F1 Binary: {eval_results['eval_f1_binary']:.4f}\")\n",
        "    print(f\"Accuracy:  {eval_results['eval_accuracy']:.4f}\")\n",
        "    print(f\"Loss:      {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "    # Detailed predictions for analysis\n",
        "    print(\"\\nGenerating detailed predictions...\")\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "    true_labels = predictions.label_ids\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(\n",
        "        true_labels,\n",
        "        pred_labels,\n",
        "        target_names=['Not Polarized', 'Polarized'],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    # Confusion matrix\n",
        "    print(\"CONFUSION MATRIX\")\n",
        "    print(\"=\"*60)\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    print(f\"                Predicted\")\n",
        "    print(f\"                Not Pol.  Polarized\")\n",
        "    print(f\"Actual Not Pol.    {cm[0][0]:4d}      {cm[0][1]:4d}\")\n",
        "    print(f\"       Polarized   {cm[1][0]:4d}      {cm[1][1]:4d}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Saving model...\")\n",
        "    model_path = \"./bitnet_polarization_final\"\n",
        "    model.bert.save_pretrained(model_path)  # Save BERT part\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': {\n",
        "            'model_name': 'bert-base-uncased',\n",
        "            'num_labels': 2,\n",
        "            'dropout_prob': 0.1\n",
        "        }\n",
        "    }, f\"{model_path}/bitnet_full_model.pt\")\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Model size analysis\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL SIZE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate effective bit-width\n",
        "    bitlinear_params = sum(p.numel() for n, p in model.named_parameters() if 'bit_fc' in n)\n",
        "    bert_params = sum(p.numel() for n, p in model.named_parameters() if 'bert' in n)\n",
        "\n",
        "    # Estimate size: BERT (16-bit) + BitLinear (1.58-bit)\n",
        "    bert_size_mb = bert_params * 2 / 1024 / 1024  # 16-bit = 2 bytes\n",
        "    bitlinear_size_mb = bitlinear_params * 1.58 / 8 / 1024 / 1024  # 1.58-bit\n",
        "    total_size_mb = bert_size_mb + bitlinear_size_mb\n",
        "\n",
        "    print(f\"BERT parameters:     {bert_params:,} ({bert_size_mb:.2f} MB)\")\n",
        "    print(f\"BitLinear parameters: {bitlinear_params:,} ({bitlinear_size_mb:.2f} MB)\")\n",
        "    print(f\"Total estimated size: {total_size_mb:.2f} MB\")\n",
        "    print(f\"Compression ratio:    {(bert_params + bitlinear_params) * 2 / 1024 / 1024 / total_size_mb:.2f}x\")\n",
        "\n",
        "    return model, tokenizer, trainer, eval_results"
      ],
      "metadata": {
        "id": "-VJaDO3Jiwgb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_polarization(text, model, tokenizer, return_probabilities=True):\n",
        "    \"\"\"\n",
        "    Make predictions on new text\n",
        "\n",
        "    Args:\n",
        "        text: Input text string\n",
        "        model: Trained BitNet model\n",
        "        tokenizer: BERT tokenizer\n",
        "        return_probabilities: If True, return probabilities along with prediction\n",
        "\n",
        "    Returns:\n",
        "        prediction: 0 (Not Polarized) or 1 (Polarized)\n",
        "        confidence: Probability of being polarized (if return_probabilities=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        pred = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][1].item()  # Probability of being polarized\n",
        "\n",
        "    if return_probabilities:\n",
        "        return pred, confidence\n",
        "    return pred"
      ],
      "metadata": {
        "id": "wqx6sTaRjAgu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_submission(model, tokenizer, test_file='subtask1/test/eng.csv', output_file='submission_subtask1.csv'):\n",
        "    \"\"\"\n",
        "    Create submission file for test set\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating submission from {test_file}...\")\n",
        "\n",
        "    # Load test data\n",
        "    test = pd.read_csv(test_file)\n",
        "    print(f\"Test samples: {len(test)}\")\n",
        "\n",
        "    # Create dataset\n",
        "    test_dataset = PolarizationDataset(\n",
        "        test['text'].tolist(),\n",
        "        [0] * len(test),  # Dummy labels\n",
        "        tokenizer,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Create trainer for prediction\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "    # Create submission file\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'polarization': pred_labels\n",
        "    })\n",
        "\n",
        "    submission.to_csv(output_file, index=False)\n",
        "    print(f\"Submission saved to {output_file}\")\n",
        "    print(f\"Prediction distribution: {pd.Series(pred_labels).value_counts().to_dict()}\")\n",
        "\n",
        "    return submission"
      ],
      "metadata": {
        "id": "Gl3b29XujFbJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference_examples(model, tokenizer):\n",
        "    \"\"\"Test model on example texts\"\"\"\n",
        "\n",
        "    test_examples = [\n",
        "        \"This politician is destroying our country with terrible policies!\",\n",
        "        \"I believe we need better education and healthcare systems.\",\n",
        "        \"Those people are all criminals and should be deported immediately!\",\n",
        "        \"Research shows that renewable energy can reduce carbon emissions.\",\n",
        "        \"They're trying to take away our rights and freedoms!\",\n",
        "        \"The weather forecast predicts rain tomorrow afternoon.\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INFERENCE EXAMPLES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, text in enumerate(test_examples, 1):\n",
        "        pred, confidence = predict_polarization(text, model, tokenizer)\n",
        "        label = \"Polarized\" if pred == 1 else \"Not Polarized\"\n",
        "        print(f\"\\n{i}. Text: {text}\")\n",
        "        print(f\"   Prediction: {label}\")\n",
        "        print(f\"   Confidence: {confidence:.3f}\")"
      ],
      "metadata": {
        "id": "ZsjWq2yAjbSv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Train the model\n",
        "    model, tokenizer, trainer, results = train_polarization_detector()\n",
        "\n",
        "    # Test on example texts\n",
        "    test_inference_examples(model, tokenizer)\n",
        "\n",
        "    # Create submission file (uncomment if you have test data)\n",
        "    # submission = create_submission(model, tokenizer)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nTo use the model for inference:\")\n",
        "    print(\"  pred, conf = predict_polarization('your text here', model, tokenizer)\")\n",
        "    print(\"\\nModel saved to: ./bitnet_polarization_final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NNUEEAaHjfNb",
        "outputId": "8e4b6245-07cd-47e2-cba7-ec23327dd1e7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SUBTASK 1: BINARY POLARIZATION DETECTION\n",
            "1-Bit LLM with BitLinear Quantization\n",
            "============================================================\n",
            "\n",
            "Loading data...\n",
            "Training samples: 2676\n",
            "Validation samples: 2676\n",
            "\n",
            "Label distribution in training:\n",
            "polarization\n",
            "0    1674\n",
            "1    1002\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class balance: {0: 0.625560538116592, 1: 0.3744394618834081}\n",
            "\n",
            "Initializing tokenizer...\n",
            "Creating datasets...\n",
            "\n",
            "Initializing BitNet model...\n",
            "Loading BERT model: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with 109,780,610 parameters\n",
            "BitLinear parameters: 298,370 (0.3% of total)\n",
            "\n",
            "Initializing BitNet trainer with lambda warmup...\n",
            "Lambda warmup enabled: 0 -> 1 over 500 steps\n",
            "\n",
            "============================================================\n",
            "Starting training...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/210 04:49, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Binary</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.532322</td>\n",
              "      <td>0.704282</td>\n",
              "      <td>0.635877</td>\n",
              "      <td>0.720105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.770500</td>\n",
              "      <td>0.373236</td>\n",
              "      <td>0.821382</td>\n",
              "      <td>0.791946</td>\n",
              "      <td>0.826233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.484800</td>\n",
              "      <td>0.270676</td>\n",
              "      <td>0.897065</td>\n",
              "      <td>0.875178</td>\n",
              "      <td>0.901719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.116308</td>\n",
              "      <td>0.958634</td>\n",
              "      <td>0.949251</td>\n",
              "      <td>0.960762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.217400</td>\n",
              "      <td>0.062781</td>\n",
              "      <td>0.977028</td>\n",
              "      <td>0.971569</td>\n",
              "      <td>0.978326</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "============================================================\n",
            "Training time: 290.40 seconds\n",
            "Training samples/second: 46.08\n",
            "\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "VALIDATION RESULTS\n",
            "============================================================\n",
            "F1 Macro:  0.9770\n",
            "F1 Binary: 0.9716\n",
            "Accuracy:  0.9783\n",
            "Loss:      0.0628\n",
            "\n",
            "Generating detailed predictions...\n",
            "\n",
            "============================================================\n",
            "CLASSIFICATION REPORT\n",
            "============================================================\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not Polarized     0.9933    0.9719    0.9825      1674\n",
            "    Polarized     0.9547    0.9890    0.9716      1002\n",
            "\n",
            "     accuracy                         0.9783      2676\n",
            "    macro avg     0.9740    0.9805    0.9770      2676\n",
            " weighted avg     0.9788    0.9783    0.9784      2676\n",
            "\n",
            "CONFUSION MATRIX\n",
            "============================================================\n",
            "                Predicted\n",
            "                Not Pol.  Polarized\n",
            "Actual Not Pol.    1627        47\n",
            "       Polarized     11       991\n",
            "\n",
            "============================================================\n",
            "Saving model...\n",
            "Model saved to ./bitnet_polarization_final\n",
            "\n",
            "============================================================\n",
            "MODEL SIZE ANALYSIS\n",
            "============================================================\n",
            "BERT parameters:     109,482,240 (208.82 MB)\n",
            "BitLinear parameters: 298,370 (0.06 MB)\n",
            "Total estimated size: 208.88 MB\n",
            "Compression ratio:    1.00x\n",
            "\n",
            "============================================================\n",
            "INFERENCE EXAMPLES\n",
            "============================================================\n",
            "\n",
            "1. Text: This politician is destroying our country with terrible policies!\n",
            "   Prediction: Polarized\n",
            "   Confidence: 0.988\n",
            "\n",
            "2. Text: I believe we need better education and healthcare systems.\n",
            "   Prediction: Not Polarized\n",
            "   Confidence: 0.018\n",
            "\n",
            "3. Text: Those people are all criminals and should be deported immediately!\n",
            "   Prediction: Polarized\n",
            "   Confidence: 0.984\n",
            "\n",
            "4. Text: Research shows that renewable energy can reduce carbon emissions.\n",
            "   Prediction: Not Polarized\n",
            "   Confidence: 0.078\n",
            "\n",
            "5. Text: They're trying to take away our rights and freedoms!\n",
            "   Prediction: Polarized\n",
            "   Confidence: 0.992\n",
            "\n",
            "6. Text: The weather forecast predicts rain tomorrow afternoon.\n",
            "   Prediction: Not Polarized\n",
            "   Confidence: 0.001\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "To use the model for inference:\n",
            "  pred, conf = predict_polarization('your text here', model, tokenizer)\n",
            "\n",
            "Model saved to: ./bitnet_polarization_final\n"
          ]
        }
      ]
    }
  ]
}